"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é€£æºå‹ãƒ‡ãƒ¼ã‚¿æº–å‚™å™¨

TimescaleDBã‹ã‚‰è¤‡æ•°æ™‚é–“è¶³ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€
ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å£²è²·ã‚·ã‚¹ãƒ†ãƒ ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ãƒ»æ•´å½¢ã™ã‚‹ã€‚
"""

import logging
import asyncio
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple

try:
    import pandas as pd
except ImportError:
    pd = None

try:
    import numpy as np
except ImportError:
    np = None

from modules.data_persistence.core.database.connection_manager import DatabaseConnectionManager
from modules.data_collection.utils.timezone_utils import TimezoneUtils
from .technical_calculator import TechnicalIndicatorCalculator


class LLMDataPreparator:
    """LLMåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™å™¨ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å£²è²·ã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.logger = logging.getLogger(__name__)
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæ–‡å­—åˆ—ã‚’è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        import os
        try:
            from dotenv import load_dotenv
            load_dotenv()
        except ImportError:
            pass  # dotenvãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‚’ç›´æ¥ä½¿ç”¨
        
        db_host = os.getenv('DB_HOST', 'localhost')
        db_port = os.getenv('DB_PORT', '5432')
        db_name = os.getenv('DB_NAME', 'trading_system')
        db_user = os.getenv('DB_USER', 'postgres')
        db_password = os.getenv('DB_PASSWORD', 'Postgres_Secure_2025!')
        
        connection_string = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
        self.connection_manager = DatabaseConnectionManager(connection_string)
        self.timezone_utils = TimezoneUtils()
        self.technical_calculator = TechnicalIndicatorCalculator()
        self._initialized = False
        
        # åˆ†æã‚¿ã‚¤ãƒ—åˆ¥è¨­å®š
        self.analysis_configs = {
            "trend_direction": {
                "timeframes": ["1d", "4h"],
                "lookback_hours": 8760,  # 1å¹´é–“ï¼ˆ365æ—¥ Ã— 24æ™‚é–“ï¼‰
                "max_data_points": 400   # 1å¹´åˆ†ã®æ—¥è¶³ãƒ‡ãƒ¼ã‚¿ç”¨
            },
            "zone_decision": {
                "timeframes": ["1h"],
                "lookback_hours": 72,   # 3æ—¥é–“
                "max_data_points": 100
            },
            "timing_execution": {
                "timeframes": ["5m"],
                "lookback_hours": 24,   # 1æ—¥é–“
                "max_data_points": 300
            },
            "trend_reinforcement": {
                "timeframes": ["5m", "15m", "1h", "4h", "1d"],
                "lookback_hours": 24,   # 1æ—¥é–“
                "max_data_points": 500
            }
        }

    async def initialize(self) -> None:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’åˆæœŸåŒ–"""
        if not self._initialized:
            try:
                await self.connection_manager.initialize()
                self._initialized = True
                self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å™¨ã®åˆæœŸåŒ–å®Œäº†")
            except Exception as e:
                self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™å™¨ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                raise

    async def prepare_analysis_data(
        self,
        analysis_type: str,
        symbol: str = 'USDJPY=X',
        timeframes: Optional[List[str]] = None
    ) -> Dict:
        """
        åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        
        Args:
            analysis_type: åˆ†æã‚¿ã‚¤ãƒ— (trend_direction, zone_decision, timing_execution, trend_reinforcement)
            symbol: é€šè²¨ãƒšã‚¢ã‚·ãƒ³ãƒœãƒ«
            timeframes: æ™‚é–“è¶³ãƒªã‚¹ãƒˆï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è¨­å®šã‹ã‚‰å–å¾—ï¼‰
        
        Returns:
            åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿è¾æ›¸
        """
        if not self._initialized:
            await self.initialize()
        self.logger.info(f"ğŸ“Š åˆ†æãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹: {analysis_type} for {symbol}")
        
        try:
            # åˆ†æè¨­å®šã®å–å¾—
            config = self.analysis_configs.get(analysis_type)
            if not config:
                raise ValueError(f"Unknown analysis type: {analysis_type}")
            
            # æ™‚é–“è¶³ã®æ±ºå®š
            if timeframes is None:
                timeframes = config["timeframes"]
            
            if timeframes is None:
                raise ValueError("timeframesãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            # ãƒ‡ãƒ¼ã‚¿å–å¾—æœŸé–“ã®è¨ˆç®—
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(hours=config["lookback_hours"])
            
            # å„æ™‚é–“è¶³ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            timeframe_data = {}
            for timeframe in timeframes:
                self.logger.info(f"â° {timeframe}è¶³ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
                raw_data = await self._fetch_timeframe_data(
                    symbol, timeframe, start_time, end_time
                )
                
                # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                is_empty = False
                try:
                    if pd is not None and hasattr(raw_data, 'empty'):
                        is_empty = getattr(raw_data, 'empty', True)
                    else:
                        is_empty = not raw_data or len(raw_data) == 0
                except (AttributeError, TypeError):
                    is_empty = not raw_data or len(raw_data) == 0
                
                if is_empty:
                    self.logger.warning(f"âš ï¸ {timeframe}è¶³ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                    continue
                
                # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
                quality_score = self._check_data_quality(raw_data, timeframe)
                
                # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—
                processed_data = self.technical_calculator.calculate_all_indicators(
                    {timeframe: raw_data}
                )[timeframe]
                
                # ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°åˆ¶é™
                if len(processed_data) > config["max_data_points"]:
                    processed_data = processed_data.tail(config["max_data_points"])
                
                timeframe_data[timeframe] = {
                    "data": processed_data,
                    "count": len(processed_data),
                    "latest": processed_data.index[-1] if not processed_data.empty else None,
                    "range": (processed_data.index[0], processed_data.index[-1]) if not processed_data.empty else None,
                    "quality_score": quality_score
                }
                
                self.logger.info(f"âœ… {timeframe}è¶³: {len(processed_data)}ä»¶, å“è³ªã‚¹ã‚³ã‚¢: {quality_score:.2f}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
            metadata = {
                "analysis_type": analysis_type,
                "symbol": symbol,
                "lookback_hours": config["lookback_hours"],
                "data_quality": np.mean([tf["quality_score"] for tf in timeframe_data.values()]) if np is not None else sum([tf["quality_score"] for tf in timeframe_data.values()]) / len(timeframe_data),
                "prepared_at": datetime.now(timezone.utc),
                "timeframes_available": list(timeframe_data.keys())
            }
            
            result = {
                "symbol": symbol,
                "timeframes": timeframe_data,
                "metadata": metadata
            }
            
            self.logger.info(f"ğŸ“Š åˆ†æãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(timeframe_data)}å€‹ã®æ™‚é–“è¶³")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def _fetch_timeframe_data(
        self,
        symbol: str,
        timeframe: str,
        start_time: datetime,
        end_time: datetime
    ):
        """
        æŒ‡å®šã•ã‚ŒãŸæ™‚é–“è¶³ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—
        
        Args:
            symbol: é€šè²¨ãƒšã‚¢ã‚·ãƒ³ãƒœãƒ«
            timeframe: æ™‚é–“è¶³
            start_time: é–‹å§‹æ™‚åˆ»
            end_time: çµ‚äº†æ™‚åˆ»
        
        Returns:
            ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®DataFrame
        """
        try:
            # SQLã‚¯ã‚¨ãƒªã®æ§‹ç¯‰ï¼ˆasyncpgç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
            query = """
            SELECT 
                timestamp,
                open,
                high,
                low,
                close,
                volume,
                data_quality_score
            FROM price_data 
            WHERE symbol = $1 
                AND timeframe = $2 
                AND timestamp >= $3 
                AND timestamp <= $4
            ORDER BY timestamp ASC
            """
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
            rows = await self.connection_manager.execute_query(
                query, symbol, timeframe, start_time, end_time
            )
            
            if not rows:
                return pd.DataFrame() if pd is not None else []
            
            if pd is None:
                # pandasãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
                return [dict(zip(['timestamp', 'open', 'high', 'low', 'close', 'volume', 'data_quality_score'], row)) for row in rows]
            
            # DataFrameã«å¤‰æ›
            df = pd.DataFrame(rows, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume', 'data_quality_score'
            ])
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’timestampã«è¨­å®š
            df.set_index('timestamp', inplace=True)
            
            # ãƒ‡ãƒ¼ã‚¿å‹ã®å¤‰æ›
            for col in ['open', 'high', 'low', 'close']:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
            df['data_quality_score'] = pd.to_numeric(df['data_quality_score'], errors='coerce')
            
            # æ¬ æå€¤ã®å‡¦ç†
            df = df.dropna()
            
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ ({timeframe}): {e}")
            return pd.DataFrame() if pd is not None else []

    def _check_data_quality(self, data, timeframe: str) -> float:
        """
        ãƒ‡ãƒ¼ã‚¿å“è³ªã®ãƒã‚§ãƒƒã‚¯
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameã¾ãŸã¯è¾æ›¸ã®ãƒªã‚¹ãƒˆï¼‰
            timeframe: æ™‚é–“è¶³
        
        Returns:
            å“è³ªã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
        if pd is None:
            # pandasãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯åŸºæœ¬çš„ãªãƒã‚§ãƒƒã‚¯ã®ã¿
            if not data or len(data) == 0:
                return 0.0
            return 0.8  # åŸºæœ¬çš„ãªå“è³ªã‚¹ã‚³ã‚¢
        
        if data.empty:
            return 0.0
        
        try:
            quality_factors = []
            
            # 1. ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
            expected_columns = ['open', 'high', 'low', 'close', 'volume']
            completeness = sum(1 for col in expected_columns if col in data.columns) / len(expected_columns)
            quality_factors.append(completeness)
            
            # 2. æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
            missing_ratio = data.isnull().sum().sum() / (len(data) * len(data.columns))
            quality_factors.append(1.0 - missing_ratio)
            
            # 3. ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            price_columns = ['open', 'high', 'low', 'close']
            valid_prices = True
            for col in price_columns:
                if col in data.columns:
                    if (data[col] <= 0).any():
                        valid_prices = False
                        break
            quality_factors.append(1.0 if valid_prices else 0.0)
            
            # 4. é«˜å€¤ãƒ»å®‰å€¤ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if all(col in data.columns for col in ['high', 'low', 'open', 'close']):
                invalid_hl = (data['high'] < data['low']).any() or \
                           (data['high'] < data['open']).any() or \
                           (data['high'] < data['close']).any() or \
                           (data['low'] > data['open']).any() or \
                           (data['low'] > data['close']).any()
                quality_factors.append(0.0 if invalid_hl else 1.0)
            else:
                quality_factors.append(0.0)
            
            # 5. ãƒ‡ãƒ¼ã‚¿å¯†åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆæ™‚é–“è¶³ã«å¿œã˜ãŸæœŸå¾…ãƒ‡ãƒ¼ã‚¿æ•°ï¼‰
            expected_intervals = {
                '5m': 12,   # 1æ™‚é–“ã«12æœ¬
                '15m': 4,   # 1æ™‚é–“ã«4æœ¬
                '1h': 1,    # 1æ™‚é–“ã«1æœ¬
                '4h': 0.25, # 1æ™‚é–“ã«0.25æœ¬
                '1d': 0.042 # 1æ™‚é–“ã«0.042æœ¬
            }
            
            if timeframe in expected_intervals:
                # æœ€æ–°24æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿å¯†åº¦ã‚’ãƒã‚§ãƒƒã‚¯
                expected_count = int(24 * expected_intervals[timeframe])
                if len(data) >= expected_count:
                    recent_data = data.tail(expected_count)
                    density_score = min(1.0, len(recent_data) / expected_count)
                    quality_factors.append(density_score)
                else:
                    # ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„å ´åˆã¯ã€åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿æ•°ã§è©•ä¾¡
                    density_score = min(1.0, len(data) / expected_count)
                    quality_factors.append(density_score)
            else:
                quality_factors.append(1.0)
            
            # ç·åˆå“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            overall_score = np.mean(quality_factors) if np is not None else sum(quality_factors) / len(quality_factors)
            
            return max(0.0, min(1.0, overall_score))
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    async def get_latest_data_summary(self, symbol: str = 'USDJPY=X') -> Dict:
        """
        æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        
        Args:
            symbol: é€šè²¨ãƒšã‚¢ã‚·ãƒ³ãƒœãƒ«
        
        Returns:
            æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®åˆæœŸåŒ–
            await self.connection_manager.initialize()
            
            timeframes = ['5m', '15m', '1h', '4h', '1d']
            summary = {}
            
            for timeframe in timeframes:
                # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                query = """
                SELECT 
                    timestamp,
                    close,
                    volume,
                    data_quality_score
                FROM price_data 
                WHERE symbol = $1 AND timeframe = $2
                ORDER BY timestamp DESC
                LIMIT 1
                """
                
                rows = await self.connection_manager.execute_query(query, symbol, timeframe)
                row = rows[0] if rows else None
                
                if row:
                    summary[timeframe] = {
                        "latest_timestamp": row[0],
                        "latest_price": float(row[1]),
                        "latest_volume": int(row[2]) if row[2] else 0,
                        "quality_score": float(row[3]) if row[3] else 0.0
                    }
                else:
                    summary[timeframe] = None
            
            return summary
            
        except Exception as e:
            self.logger.error(f"âŒ æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    async def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        await self.connection_manager.close()
        self._initialized = False


# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
async def main():
    """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    preparator = LLMDataPreparator()
    
    try:
        # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        print("ğŸ§ª ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ†ã‚¹ãƒˆ...")
        trend_data = await preparator.prepare_analysis_data("trend_direction")
        print(f"âœ… ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(trend_data['timeframes'])}å€‹ã®æ™‚é–“è¶³")
        
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã®å–å¾—
        print("\nğŸ§ª æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ãƒ†ã‚¹ãƒˆ...")
        summary = await preparator.get_latest_data_summary()
        for tf, data in summary.items():
            if data:
                print(f"  {tf}: {data['latest_price']:.5f} @ {data['latest_timestamp']}")
            else:
                print(f"  {tf}: ãƒ‡ãƒ¼ã‚¿ãªã—")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        await preparator.close()


if __name__ == "__main__":
    asyncio.run(main())
