#!/usr/bin/env python3
"""
ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾å­˜é–¢ä¿‚åˆ†æãƒ„ãƒ¼ãƒ«

ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€Exchange Analytics Systemã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä¾å­˜é–¢ä¿‚ã¨ä½¿ç”¨çŠ¶æ³ã‚’åˆ†æã—ã€
ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã®å®‰å…¨æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã®æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import re
import json
import argparse
from pathlib import Path
from typing import Dict, List
from collections import defaultdict
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ScriptAnalyzer:
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä¾å­˜é–¢ä¿‚ã¨ä½¿ç”¨çŠ¶æ³ã‚’åˆ†æã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, root_path: str = "/app"):
        self.root_path = Path(root_path)
        self.scripts_dir = self.root_path / "scripts"
        self.src_dir = self.root_path / "src"
        self.tests_dir = self.root_path / "tests"
        
        # åˆ†æçµæœ
        self.imports_map = defaultdict(set)  # ãƒ•ã‚¡ã‚¤ãƒ« -> ã‚¤ãƒ³ãƒãƒ¼ãƒˆå…ˆ
        self.imported_by_map = defaultdict(set)  # ãƒ•ã‚¡ã‚¤ãƒ« -> ã‚¤ãƒ³ãƒãƒ¼ãƒˆå…ƒ
        self.usage_map = defaultdict(set)  # ãƒ•ã‚¡ã‚¤ãƒ« -> ä½¿ç”¨ç®‡æ‰€
        self.risk_assessment = {}  # ãƒ•ã‚¡ã‚¤ãƒ« -> ãƒªã‚¹ã‚¯è©•ä¾¡
        
    def analyze_all_scripts(self) -> Dict:
        """å…¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®åˆ†æã‚’å®Ÿè¡Œ"""
        logger.info("ã‚¹ã‚¯ãƒªãƒ—ãƒˆåˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        
        # å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åˆ†æ
        self._analyze_directory(self.scripts_dir, "scripts")
        self._analyze_directory(self.src_dir, "src")
        self._analyze_directory(self.tests_dir, "tests")
        
        # ä½¿ç”¨çŠ¶æ³ã®åˆ†æ
        self._analyze_usage_patterns()
        
        # ãƒªã‚¹ã‚¯è©•ä¾¡
        self._assess_risks()
        
        return self._generate_report()
    
    def _analyze_directory(self, directory: Path, dir_type: str):
        """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
        if not directory.exists():
            logger.warning(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {directory}")
            return
            
        for py_file in directory.rglob("*.py"):
            logger.info(f"åˆ†æä¸­: {py_file}")
            self._analyze_python_file(py_file, dir_type)
    
    def _analyze_python_file(self, file_path: Path, dir_type: str):
        """Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾å­˜é–¢ä¿‚ã‚’åˆ†æ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã®æŠ½å‡º
            imports = self._extract_imports(content)
            
            # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
            absolute_imports = self._resolve_imports(imports, file_path)
            
            # ãƒãƒƒãƒ—ã«è¿½åŠ 
            relative_path = str(file_path.relative_to(self.root_path))
            self.imports_map[relative_path] = absolute_imports
            
            for imported_file in absolute_imports:
                self.imported_by_map[imported_file].add(relative_path)
                
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
    
    def _extract_imports(self, content: str) -> List[str]:
        """Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã‚’æŠ½å‡º"""
        imports = []
        
        # æ¨™æº–çš„ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡
        import_patterns = [
            r'^import\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s*$',
            r'^from\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s+import',
            r'^from\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s+import\s+([a-zA-Z_][a-zA-Z0-9_,\s]*)',
        ]
        
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('#') or not line:
                continue
                
            for pattern in import_patterns:
                match = re.match(pattern, line)
                if match:
                    imports.append(match.group(1))
                    break
        
        # å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        dynamic_patterns = [
            r'__import__\s*\(\s*["\']([^"\']+)["\']',
            r'importlib\.import_module\s*\(\s*["\']([^"\']+)["\']',
        ]
        
        for pattern in dynamic_patterns:
            matches = re.findall(pattern, content)
            imports.extend(matches)
        
        return list(set(imports))
    
    def _resolve_imports(self, imports: List[str], source_file: Path) -> List[str]:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«è§£æ±º"""
        resolved = []
        
        for imp in imports:
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            if imp.startswith('scripts.'):
                script_path = self.root_path / imp.replace('.', '/') + '.py'
                if script_path.exists():
                    resolved.append(str(script_path.relative_to(self.root_path)))
            
            # srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            elif imp.startswith('src.'):
                src_path = self.root_path / imp.replace('.', '/') + '.py'
                if src_path.exists():
                    resolved.append(str(src_path.relative_to(self.root_path)))
            
            # ãã®ä»–ã®ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            elif not imp.startswith(('.', '/')):
                # æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚„ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯é™¤å¤–
                if not self._is_standard_library(imp):
                    # ç›¸å¯¾ãƒ‘ã‚¹ã§è§£æ±ºã‚’è©¦è¡Œ
                    possible_paths = [
                        source_file.parent / f"{imp}.py",
                        source_file.parent / imp / "__init__.py",
                        self.scripts_dir / f"{imp}.py",
                        self.src_dir / f"{imp}.py",
                    ]
                    
                    for path in possible_paths:
                        if path.exists():
                            resolved.append(str(path.relative_to(self.root_path)))
                            break
        
        return resolved
    
    def _is_standard_library(self, module_name: str) -> bool:
        """æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        standard_modules = {
            'os', 'sys', 're', 'json', 'logging', 'pathlib', 'typing',
            'collections', 'datetime', 'time', 'random', 'math', 'statistics',
            'itertools', 'functools', 'argparse', 'subprocess', 'shutil',
            'glob', 'fnmatch', 'tempfile', 'pickle', 'sqlite3', 'csv',
            'xml', 'html', 'urllib', 'requests', 'threading', 'multiprocessing',
            'asyncio', 'concurrent', 'queue', 'socket', 'ssl', 'hashlib',
            'base64', 'zlib', 'gzip', 'bz2', 'lzma', 'tarfile', 'zipfile',
            'configparser', 'logging', 'getpass', 'platform', 'psutil'
        }
        
        return module_name.split('.')[0] in standard_modules
    
    def _analyze_usage_patterns(self):
        """ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        logger.info("ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æã‚’é–‹å§‹...")
        
        # cronã‚¸ãƒ§ãƒ–ã§ã®ä½¿ç”¨çŠ¶æ³
        self._analyze_cron_usage()
        
        # ãƒ†ã‚¹ãƒˆã§ã®ä½¿ç”¨çŠ¶æ³
        self._analyze_test_usage()
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ã®å‚ç…§
        self._analyze_config_references()
    
    def _analyze_cron_usage(self):
        """cronã‚¸ãƒ§ãƒ–ã§ã®ä½¿ç”¨çŠ¶æ³ã‚’åˆ†æ"""
        crontab_file = self.root_path / "crontab_new.txt"
        if crontab_file.exists():
            with open(crontab_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œã‚’æ¤œç´¢
            python_scripts = re.findall(r'python\s+([^\s]+\.py)', content)
            for script in python_scripts:
                if script.startswith('scripts/'):
                    script_path = script
                    self.usage_map[script_path].add('cron')
                    logger.info(f"cronã§ä½¿ç”¨: {script_path}")
    
    def _analyze_test_usage(self):
        """ãƒ†ã‚¹ãƒˆã§ã®ä½¿ç”¨çŠ¶æ³ã‚’åˆ†æ"""
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å†…ã§ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆå‚ç…§
        for test_file in self.tests_dir.rglob("*.py"):
            try:
                with open(test_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # scriptsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’æ¤œç´¢
                script_imports = re.findall(r'from\s+scripts\.([^\s]+)', content)
                for script_import in script_imports:
                    script_path = f"scripts/{script_import.replace('.', '/')}.py"
                    self.usage_map[script_path].add('test')
                    
            except Exception as e:
                logger.error(f"ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {test_file}: {e}")
    
    def _analyze_config_references(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ã®å‚ç…§ã‚’åˆ†æ"""
        config_files = [
            self.root_path / "README.md",
            self.root_path / "scripts" / "README.md",
            self.root_path / "src" / "README.md",
        ]
        
        for config_file in config_files:
            if config_file.exists():
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å‚ç…§ã‚’æ¤œç´¢
                    script_refs = re.findall(r'scripts/([^\s]+\.py)', content)
                    for script_ref in script_refs:
                        script_path = f"scripts/{script_ref}"
                        self.usage_map[script_path].add('documentation')
                        
                except Exception as e:
                    logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {config_file}: {e}")
    
    def _assess_risks(self):
        """å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡"""
        logger.info("ãƒªã‚¹ã‚¯è©•ä¾¡ã‚’é–‹å§‹...")
        
        for file_path in self.imports_map.keys():
            risk_level = self._calculate_risk_level(file_path)
            self.risk_assessment[file_path] = risk_level
    
    def _calculate_risk_level(self, file_path: str) -> Dict:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—"""
        risk_score = 0
        risk_factors = []
        
        # ä½¿ç”¨çŠ¶æ³ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯
        usage = self.usage_map.get(file_path, set())
        if 'cron' in usage:
            risk_score += 10
            risk_factors.append("cronã‚¸ãƒ§ãƒ–ã§ä½¿ç”¨")
        if 'test' in usage:
            risk_score += 3
            risk_factors.append("ãƒ†ã‚¹ãƒˆã§ä½¿ç”¨")
        if 'documentation' in usage:
            risk_score += 2
            risk_factors.append("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§å‚ç…§")
        
        # ä¾å­˜é–¢ä¿‚ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯
        imported_by = self.imported_by_map.get(file_path, set())
        if imported_by:
            risk_score += len(imported_by) * 2
            risk_factors.append(f"{len(imported_by)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯
        if file_path.startswith('scripts/cron/'):
            risk_score += 5
            risk_factors.append("cronãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
        elif file_path.startswith('scripts/monitoring/'):
            risk_score += 5
            risk_factors.append("monitoringãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
        elif file_path.startswith('scripts/deployment/'):
            risk_score += 5
            risk_factors.append("deploymentãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
        elif file_path.startswith('scripts/archive/'):
            risk_score += 1
            risk_factors.append("archiveãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
        
        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š
        if risk_score >= 15:
            risk_level = "é«˜"
        elif risk_score >= 8:
            risk_level = "ä¸­"
        else:
            risk_level = "ä½"
        
        return {
            "risk_score": risk_score,
            "risk_level": risk_level,
            "risk_factors": risk_factors,
            "usage": list(usage),
            "imported_by": list(imported_by)
        }
    
    def _generate_report(self) -> Dict:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        logger.info("åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        
        # çµ±è¨ˆæƒ…å ±
        total_files = len(self.imports_map)
        high_risk_files = len([f for f, r in self.risk_assessment.items() if r['risk_level'] == 'é«˜'])
        medium_risk_files = len([f for f, r in self.risk_assessment.items() if r['risk_level'] == 'ä¸­'])
        low_risk_files = len([f for f, r in self.risk_assessment.items() if r['risk_level'] == 'ä½'])
        
        # å‰Šé™¤æ¨å¥¨ãƒ•ã‚¡ã‚¤ãƒ«
        safe_to_delete = [
            f for f, r in self.risk_assessment.items()
            if (r['risk_level'] == 'ä½' and not r['usage'] 
                and not r['imported_by'])
        ]

        # æ³¨æ„ãŒå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«
        high_attention = [
            f for f, r in self.risk_assessment.items()
            if r['risk_level'] == 'é«˜'
        ]
        
        report = {
            "summary": {
                "total_files": total_files,
                "high_risk": high_risk_files,
                "medium_risk": medium_risk_files,
                "low_risk": low_risk_files,
                "safe_to_delete": len(safe_to_delete)
            },
            "risk_assessment": self.risk_assessment,
            "safe_to_delete_files": safe_to_delete,
            "high_attention_files": high_attention,
            "imports_map": dict(self.imports_map),
            "usage_map": dict(self.usage_map)
        }
        
        return report
    
    def save_report(self, report: Dict, output_file: str = "script_analysis_report.json"):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output_path = self.root_path / "scripts" / "refactoring" / output_file
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    
    def print_summary(self, report: Dict):
        """ãƒ¬ãƒãƒ¼ãƒˆã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        summary = report['summary']
        
        print("\n" + "="*60)
        print("ğŸ“Š ã‚¹ã‚¯ãƒªãƒ—ãƒˆåˆ†æçµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        print(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {summary['total_files']}")
        print(f"é«˜ãƒªã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«: {summary['high_risk']}")
        print(f"ä¸­ãƒªã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«: {summary['medium_risk']}")
        print(f"ä½ãƒªã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«: {summary['low_risk']}")
        print(f"å®‰å…¨ã«å‰Šé™¤å¯èƒ½: {summary['safe_to_delete']}")
        
        print("\n" + "-"*60)
        print("âš ï¸  é«˜ãƒªã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå‰Šé™¤ä¸å¯ï¼‰")
        print("-"*60)
        for file_path in report['high_attention_files']:
            risk_info = report['risk_assessment'][file_path]
            print(f"â€¢ {file_path} (ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: {risk_info['risk_score']})")
            print(f"  ç†ç”±: {', '.join(risk_info['risk_factors'])}")
        
        print("\n" + "-"*60)
        print("ğŸ—‘ï¸  å®‰å…¨ã«å‰Šé™¤å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«")
        print("-"*60)
        for file_path in report['safe_to_delete_files']:
            print(f"â€¢ {file_path}")
        
        print("\n" + "-"*60)
        print("ğŸ“ æ¨å¥¨äº‹é …")
        print("-"*60)
        print("1. é«˜ãƒªã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã¯çµ¶å¯¾ã«å‰Šé™¤ã—ãªã„ã§ãã ã•ã„")
        print("2. ä¸­ãƒªã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ…é‡ã«æ‰±ã„ã€äº‹å‰ã«ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„")
        print("3. ä½ãƒªã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ®µéšçš„ã«å‰Šé™¤ã—ã¦ãã ã•ã„")
        print("4. å‰Šé™¤å‰ã«ã¯å¿…ãšãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å–ã£ã¦ãã ã•ã„")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾å­˜é–¢ä¿‚åˆ†æãƒ„ãƒ¼ãƒ«")
    parser.add_argument(
        "--root-path", 
        default="/app", 
        help="åˆ†æå¯¾è±¡ã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: /app)"
    )
    parser.add_argument(
        "--output", 
        default="script_analysis_report.json",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: script_analysis_report.json)"
    )
    parser.add_argument(
        "--verbose", 
        action="store_true",
        help="è©³ç´°ãªãƒ­ã‚°ã‚’å‡ºåŠ›"
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ†æã®å®Ÿè¡Œ
    analyzer = ScriptAnalyzer(args.root_path)
    report = analyzer.analyze_all_scripts()
    
    # ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜
    analyzer.save_report(report, args.output)
    
    # ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
    analyzer.print_summary(report)
    
    logger.info("åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚è©³ç´°ã¯ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()
