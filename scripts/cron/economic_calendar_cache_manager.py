#!/usr/bin/env python3
"""
çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
APIå–å¾—æ•°ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½
"""
import asyncio
import logging
import os
import sys
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.infrastructure.database.connection import get_async_session
from src.infrastructure.database.repositories.analysis_cache_repository_impl import (
    AnalysisCacheRepositoryImpl,
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class EconomicCalendarCacheManager:
    """çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.session = None
        self.analysis_cache_repo = None

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.cache_ttl_hours = {
            "daily": 6,  # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿: 6æ™‚é–“
            "weekly": 24,  # é€±æ¬¡ãƒ‡ãƒ¼ã‚¿: 24æ™‚é–“
            "monthly": 168,  # æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿: 1é€±é–“
        }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
        self.cache_stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "api_calls_saved": 0,
            "last_cache_cleanup": None,
        }

    async def initialize(self):
        """åˆæœŸåŒ–"""
        try:
            self.session = await get_async_session()
            self.analysis_cache_repo = AnalysisCacheRepositoryImpl(self.session)

            logger.info("âœ… çµŒæ¸ˆæŒ‡æ¨™ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def close(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒ­ãƒ¼ã‚º"""
        if self.session:
            await self.session.close()

    async def get_cached_economic_events(
        self,
        from_date: str,
        to_date: str,
        countries: List[str],
        importances: List[str],
        cache_type: str = "daily",
    ) -> Optional[List[Dict[str, Any]]]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            from_date: é–‹å§‹æ—¥ï¼ˆDD/MM/YYYYï¼‰
            to_date: çµ‚äº†æ—¥ï¼ˆDD/MM/YYYYï¼‰
            countries: å¯¾è±¡å›½ãƒªã‚¹ãƒˆ
            importances: é‡è¦åº¦ãƒªã‚¹ãƒˆ
            cache_type: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¿ã‚¤ãƒ—ï¼ˆdaily/weekly/monthlyï¼‰

        Returns:
            Optional[List[Dict[str, Any]]]: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã€ãªã‘ã‚Œã°None
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
            cache_key = self._generate_cache_key(
                from_date, to_date, countries, importances, cache_type
            )

            # åˆ†æã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¤œç´¢
            cached_analysis = await self.analysis_cache_repo.find_by_cache_key(
                cache_key
            )

            if cached_analysis and not self._is_cache_expired(
                cached_analysis, cache_type
            ):
                self.cache_stats["cache_hits"] += 1
                logger.info(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {cache_key}")

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                return self._parse_cached_data(cached_analysis.analysis_data)
            else:
                self.cache_stats["cache_misses"] += 1
                logger.info(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: {cache_key}")
                return None

        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    async def save_economic_events_cache(
        self,
        events: List[Dict[str, Any]],
        from_date: str,
        to_date: str,
        countries: List[str],
        importances: List[str],
        cache_type: str = "daily",
    ) -> bool:
        """
        çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜

        Args:
            events: çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿
            from_date: é–‹å§‹æ—¥
            to_date: çµ‚äº†æ—¥
            countries: å¯¾è±¡å›½ãƒªã‚¹ãƒˆ
            importances: é‡è¦åº¦ãƒªã‚¹ãƒˆ
            cache_type: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¿ã‚¤ãƒ—

        Returns:
            bool: ä¿å­˜æˆåŠŸæ™‚True
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
            cache_key = self._generate_cache_key(
                from_date, to_date, countries, importances, cache_type
            )

                        # æœ‰åŠ¹æœŸé™ã‚’è¨ˆç®—
            expires_at = self._calculate_expires_at(cache_type)
            
            # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç¢ºèª
            existing_cache = await self.analysis_cache_repo.find_by_cache_key(cache_key)
            
            if existing_cache:
                # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å±æ€§ã‚’ç›´æ¥æ›´æ–°
                existing_cache.analysis_data = self._serialize_events_data(events)
                existing_cache.expires_at = expires_at
                existing_cache.version = (existing_cache.version or 0) + 1
                
                await self.analysis_cache_repo.save(existing_cache)
                logger.info(f"âœ… çµŒæ¸ˆæŒ‡æ¨™ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°: {cache_key} ({len(events)}ä»¶)")
            else:
                # æ–°è¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆ
                from src.domain.entities.analysis_cache import AnalysisCache
                
                cache_entity = AnalysisCache(
                    cache_key=cache_key,
                    analysis_type="economic_calendar",
                    currency_pair="ALL",  # å…¨é€šè²¨ãƒšã‚¢å¯¾è±¡
                    analysis_data=self._serialize_events_data(events),
                    expires_at=expires_at,
                )
                
                await self.analysis_cache_repo.save(cache_entity)
                logger.info(f"âœ… çµŒæ¸ˆæŒ‡æ¨™ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {cache_key} ({len(events)}ä»¶)")

            return True

        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    async def get_cached_weekly_events(
        self, start_date: str, countries: List[str], importances: List[str]
    ) -> Optional[List[Dict[str, Any]]]:
        """
        é€±æ¬¡çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—

        Args:
            start_date: é–‹å§‹æ—¥ï¼ˆDD/MM/YYYYï¼‰
            countries: å¯¾è±¡å›½ãƒªã‚¹ãƒˆ
            importances: é‡è¦åº¦ãƒªã‚¹ãƒˆ

        Returns:
            Optional[List[Dict[str, Any]]]: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        # é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã®çµ‚äº†æ—¥ã‚’è¨ˆç®—
        start_dt = datetime.strptime(start_date, "%d/%m/%Y")
        end_dt = start_dt + timedelta(days=6)
        end_date = end_dt.strftime("%d/%m/%Y")

        return await self.get_cached_economic_events(
            start_date, end_date, countries, importances, "weekly"
        )

    async def save_weekly_events_cache(
        self,
        events: List[Dict[str, Any]],
        start_date: str,
        countries: List[str],
        importances: List[str],
    ) -> bool:
        """
        é€±æ¬¡çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜

        Args:
            events: çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿
            start_date: é–‹å§‹æ—¥
            countries: å¯¾è±¡å›½ãƒªã‚¹ãƒˆ
            importances: é‡è¦åº¦ãƒªã‚¹ãƒˆ

        Returns:
            bool: ä¿å­˜æˆåŠŸæ™‚True
        """
        # é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã®çµ‚äº†æ—¥ã‚’è¨ˆç®—
        start_dt = datetime.strptime(start_date, "%d/%m/%Y")
        end_dt = start_dt + timedelta(days=6)
        end_date = end_dt.strftime("%d/%m/%Y")

        return await self.save_economic_events_cache(
            events, start_date, end_date, countries, importances, "weekly"
        )

    async def cleanup_expired_cache(self) -> int:
        """
        æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤

        Returns:
            int: å‰Šé™¤ã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥æ•°
        """
        try:
            deleted_count = await self.analysis_cache_repo.delete_expired()
            self.cache_stats["last_cache_cleanup"] = datetime.now()

            logger.info(f"ğŸ—‘ï¸ æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤: {deleted_count}ä»¶")
            return deleted_count

        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    async def get_cache_statistics(self) -> Dict[str, Any]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—

        Returns:
            Dict[str, Any]: ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
        """
        try:
            # æœ‰åŠ¹ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥æ•°ã‚’å–å¾—
            valid_caches = await self.analysis_cache_repo.find_valid_caches(
                "economic_calendar", "ALL"
            )

            stats = {
                **self.cache_stats,
                "valid_cache_count": len(valid_caches),
                "cache_hit_rate": (
                    self.cache_stats["cache_hits"]
                    / (
                        self.cache_stats["cache_hits"]
                        + self.cache_stats["cache_misses"]
                    )
                    if (
                        self.cache_stats["cache_hits"]
                        + self.cache_stats["cache_misses"]
                    )
                    > 0
                    else 0
                ),
                "estimated_api_calls_saved": self.cache_stats["api_calls_saved"],
            }

            return stats

        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return self.cache_stats

    def _generate_cache_key(
        self,
        from_date: str,
        to_date: str,
        countries: List[str],
        importances: List[str],
        cache_type: str,
    ) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        countries_str = "_".join(sorted(countries))
        importances_str = "_".join(sorted(importances))

        return f"economic_calendar_{cache_type}_{from_date}_{to_date}_{countries_str}_{importances_str}"

    def _is_cache_expired(self, cache_entity, cache_type: str) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœŸé™åˆ‡ã‚Œã‹ãƒã‚§ãƒƒã‚¯"""
        if not cache_entity or not cache_entity.expires_at:
            return True
            
        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’é™¤å»ã—ã¦UTCã§æ¯”è¼ƒ
        now = datetime.utcnow()
        expires_at = cache_entity.expires_at
        
        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’é™¤å»ã—ã¦æ¯”è¼ƒ
        if expires_at.tzinfo is not None:
            expires_at = expires_at.replace(tzinfo=None)
        if now.tzinfo is not None:
            now = now.replace(tzinfo=None)
            
        return now > expires_at

    def _calculate_expires_at(self, cache_type: str) -> datetime:
        """æœ‰åŠ¹æœŸé™ã‚’è¨ˆç®—"""
        ttl_hours = self.cache_ttl_hours.get(cache_type, 24)
        return datetime.utcnow() + timedelta(hours=ttl_hours)

    def _serialize_events_data(self, events: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        return {
            "events": events,
            "cached_at": datetime.utcnow().isoformat(),
            "event_count": len(events),
        }

    def _parse_cached_data(self, cache_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹"""
        if isinstance(cache_data, dict) and "events" in cache_data:
            return cache_data["events"]
        elif isinstance(cache_data, list):
            return cache_data
        else:
            logger.warning("âš ï¸ ä¸æ˜ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å½¢å¼")
            return []


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="çµŒæ¸ˆæŒ‡æ¨™ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument(
        "--cleanup", action="store_true", help="æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤"
    )
    parser.add_argument("--stats", action="store_true", help="ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’è¡¨ç¤º")
    parser.add_argument("--test", action="store_true", help="ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")

    args = parser.parse_args()

    cache_manager = EconomicCalendarCacheManager()

    try:
        await cache_manager.initialize()

        if args.cleanup:
            # æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤
            deleted_count = await cache_manager.cleanup_expired_cache()
            logger.info(f"ğŸ—‘ï¸ æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤å®Œäº†: {deleted_count}ä»¶")

        elif args.stats:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆè¡¨ç¤º
            stats = await cache_manager.get_cache_statistics()
            logger.info(f"ğŸ“Š ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ: {stats}")

        elif args.test:
            # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œ
            test_events = [
                {
                    "date": "2025-08-25",
                    "time": "08:30",
                    "country": "japan",
                    "event": "Consumer Price Index (CPI)",
                    "importance": "high",
                    "currency": "JPY",
                    "actual": None,
                    "forecast": 2.5,
                    "previous": 2.3,
                }
            ]

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ãƒ†ã‚¹ãƒˆ
            success = await cache_manager.save_weekly_events_cache(
                test_events,
                "25/08/2025",
                ["japan", "united states"],
                ["high", "medium"],
            )

            if success:
                logger.info("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ãƒ†ã‚¹ãƒˆæˆåŠŸ")

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ãƒ†ã‚¹ãƒˆ
            cached_data = await cache_manager.get_cached_weekly_events(
                "25/08/2025", ["japan", "united states"], ["high", "medium"]
            )

            if cached_data:
                logger.info(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(cached_data)}ä»¶")

            # çµ±è¨ˆå–å¾—
            stats = await cache_manager.get_cache_statistics()
            logger.info(f"ğŸ“Š ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ: {stats}")

        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: çµ±è¨ˆè¡¨ç¤º
            stats = await cache_manager.get_cache_statistics()
            logger.info(f"ğŸ“Š ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ: {stats}")

    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        await cache_manager.close()


if __name__ == "__main__":
    asyncio.run(main())
