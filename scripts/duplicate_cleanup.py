#!/usr/bin/env python3
"""
é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºãƒ»å‰Šé™¤ã—ã¾ã™ã€‚
"""

import asyncio
import logging
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from modules.data_persistence.config.settings import DataPersistenceSettings
from modules.data_persistence.core.database.connection_manager import (
    DatabaseConnectionManager,
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DuplicateCleanupService:
    """é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒ“ã‚¹"""

    def __init__(self, connection_manager: DatabaseConnectionManager):
        self.connection_manager = connection_manager

    async def analyze_duplicates(self) -> Dict[str, Any]:
        """é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ"""
        try:
            # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®ä»¶æ•°ã‚’ç¢ºèª
            query = """
                SELECT 
                    symbol,
                    timeframe,
                    COUNT(*) as total_records,
                    COUNT(DISTINCT timestamp) as unique_timestamps,
                    COUNT(*) - COUNT(DISTINCT timestamp) as duplicate_count
                FROM price_data 
                GROUP BY symbol, timeframe
                HAVING COUNT(*) > COUNT(DISTINCT timestamp)
                ORDER BY duplicate_count DESC
            """

            result = await self.connection_manager.execute_query(query)

            analysis = {
                "duplicate_groups": [],
                "total_duplicates": 0,
                "affected_symbols": set(),
                "affected_timeframes": set(),
            }

            for row in result:
                group_info = {
                    "symbol": row["symbol"],
                    "timeframe": row["timeframe"],
                    "total_records": row["total_records"],
                    "unique_timestamps": row["unique_timestamps"],
                    "duplicate_count": row["duplicate_count"],
                }
                analysis["duplicate_groups"].append(group_info)
                analysis["total_duplicates"] += row["duplicate_count"]
                analysis["affected_symbols"].add(row["symbol"])
                analysis["affected_timeframes"].add(row["timeframe"])

            analysis["affected_symbols"] = list(analysis["affected_symbols"])
            analysis["affected_timeframes"] = list(analysis["affected_timeframes"])

            return analysis

        except Exception as e:
            logger.error(f"Failed to analyze duplicates: {e}")
            raise

    async def cleanup_duplicates(self, dry_run: bool = True) -> Dict[str, Any]:
        """é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ã‚’å–å¾—
            query = """
                SELECT 
                    symbol,
                    timeframe,
                    timestamp,
                    COUNT(*) as count,
                    MIN(id) as min_id,
                    MAX(id) as max_id,
                    ARRAY_AGG(id ORDER BY id) as all_ids
                FROM price_data 
                GROUP BY symbol, timeframe, timestamp
                HAVING COUNT(*) > 1
                ORDER BY symbol, timeframe, timestamp
            """

            result = await self.connection_manager.execute_query(query)

            cleanup_stats = {
                "total_duplicate_groups": len(result),
                "total_records_to_delete": 0,
                "records_to_keep": 0,
                "cleanup_details": [],
            }

            if dry_run:
                logger.info("ğŸ” DRY RUN MODE - å®Ÿéš›ã®å‰Šé™¤ã¯è¡Œã„ã¾ã›ã‚“")

            for row in result:
                symbol = row["symbol"]
                timeframe = row["timeframe"]
                timestamp = row["timestamp"]
                count = row["count"]
                all_ids = row["all_ids"]

                # æœ€æ–°ã®IDã‚’æ®‹ã™ï¼ˆæœ€å¤§IDï¼‰
                ids_to_delete = all_ids[:-1]  # æœ€å¾Œã®IDä»¥å¤–ã‚’å‰Šé™¤
                id_to_keep = all_ids[-1]  # æœ€æ–°ã®IDã‚’ä¿æŒ

                cleanup_detail = {
                    "symbol": symbol,
                    "timeframe": timeframe,
                    "timestamp": timestamp,
                    "duplicate_count": count,
                    "ids_to_delete": ids_to_delete,
                    "id_to_keep": id_to_keep,
                }
                cleanup_stats["cleanup_details"].append(cleanup_detail)
                cleanup_stats["total_records_to_delete"] += len(ids_to_delete)
                cleanup_stats["records_to_keep"] += 1

                if not dry_run:
                    # å®Ÿéš›ã«å‰Šé™¤ã‚’å®Ÿè¡Œ
                    delete_query = """
                        DELETE FROM price_data 
                        WHERE id = ANY($1)
                    """
                    await self.connection_manager.execute_command(
                        delete_query, ids_to_delete
                    )
                    logger.info(
                        f"Deleted {len(ids_to_delete)} duplicate records for {symbol} {timeframe} at {timestamp}"
                    )

            if not dry_run:
                logger.info(
                    f"âœ… Cleanup completed: {cleanup_stats['total_records_to_delete']} records deleted"
                )
            else:
                logger.info(
                    f"ğŸ” DRY RUN: Would delete {cleanup_stats['total_records_to_delete']} records"
                )

            return cleanup_stats

        except Exception as e:
            logger.error(f"Failed to cleanup duplicates: {e}")
            raise

    async def verify_cleanup(self) -> Dict[str, Any]:
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®æ¤œè¨¼"""
        try:
            # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã£ã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            query = """
                SELECT 
                    symbol,
                    timeframe,
                    timestamp,
                    COUNT(*) as count
                FROM price_data 
                GROUP BY symbol, timeframe, timestamp
                HAVING COUNT(*) > 1
            """

            result = await self.connection_manager.execute_query(query)

            if len(result) == 0:
                logger.info("âœ… é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã¯å®Œå…¨ã«å‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
                return {"status": "clean", "remaining_duplicates": 0}
            else:
                logger.warning(f"âš ï¸ {len(result)}å€‹ã®é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ãŒæ®‹ã£ã¦ã„ã¾ã™")
                return {"status": "dirty", "remaining_duplicates": len(result)}

        except Exception as e:
            logger.error(f"Failed to verify cleanup: {e}")
            raise

    async def get_data_quality_report(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        try:
            # å…¨ä½“ã®çµ±è¨ˆ
            total_query = """
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT symbol) as unique_symbols,
                    COUNT(DISTINCT timeframe) as unique_timeframes,
                    MIN(timestamp) as earliest_data,
                    MAX(timestamp) as latest_data
                FROM price_data
            """

            total_result = await self.connection_manager.execute_query(total_query)
            total_stats = dict(total_result[0]) if total_result else {}

            # æ™‚é–“è¶³åˆ¥ã®çµ±è¨ˆ
            timeframe_query = """
                SELECT 
                    timeframe,
                    COUNT(*) as record_count,
                    COUNT(DISTINCT symbol) as symbol_count,
                    MIN(timestamp) as earliest,
                    MAX(timestamp) as latest
                FROM price_data
                GROUP BY timeframe
                ORDER BY timeframe
            """

            timeframe_result = await self.connection_manager.execute_query(
                timeframe_query
            )
            timeframe_stats = [dict(row) for row in timeframe_result]

            # ã‚·ãƒ³ãƒœãƒ«åˆ¥ã®çµ±è¨ˆ
            symbol_query = """
                SELECT 
                    symbol,
                    COUNT(*) as record_count,
                    COUNT(DISTINCT timeframe) as timeframe_count,
                    MIN(timestamp) as earliest,
                    MAX(timestamp) as latest
                FROM price_data
                GROUP BY symbol
                ORDER BY symbol
            """

            symbol_result = await self.connection_manager.execute_query(symbol_query)
            symbol_stats = [dict(row) for row in symbol_result]

            return {
                "total_stats": total_stats,
                "timeframe_stats": timeframe_stats,
                "symbol_stats": symbol_stats,
                "generated_at": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"Failed to generate data quality report: {e}")
            raise


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument(
        "--action",
        choices=["analyze", "cleanup", "verify", "report"],
        default="analyze",
        help="å®Ÿè¡Œã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="å®Ÿéš›ã®å‰Šé™¤ã‚’è¡Œã‚ãšã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿å®Ÿè¡Œ",
    )
    parser.add_argument("--force", action="store_true", help="ç¢ºèªãªã—ã§å‰Šé™¤ã‚’å®Ÿè¡Œ")

    args = parser.parse_args()

    try:
        # è¨­å®šã‚’èª­ã¿è¾¼ã¿
        settings = DataPersistenceSettings.from_env()
        logger.info(f"Connecting to database: {settings.database.database}")

        # æ¥ç¶šç®¡ç†ã‚’åˆæœŸåŒ–
        connection_manager = DatabaseConnectionManager(
            connection_string=settings.database.connection_string,
            min_connections=settings.database.min_connections,
            max_connections=settings.database.max_connections,
        )

        await connection_manager.initialize()

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
        cleanup_service = DuplicateCleanupService(connection_manager)

        if args.action == "analyze":
            logger.info("ğŸ” é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã„ã¾ã™...")
            analysis = await cleanup_service.analyze_duplicates()

            if analysis["total_duplicates"] == 0:
                logger.info("âœ… é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                logger.info(
                    f"âš ï¸ {analysis['total_duplicates']}ä»¶ã®é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ"
                )
                logger.info(
                    f"å½±éŸ¿ã‚’å—ã‘ã‚‹ã‚·ãƒ³ãƒœãƒ«: {', '.join(analysis['affected_symbols'])}"
                )
                logger.info(
                    f"å½±éŸ¿ã‚’å—ã‘ã‚‹æ™‚é–“è¶³: {', '.join(analysis['affected_timeframes'])}"
                )

                for group in analysis["duplicate_groups"]:
                    logger.info(
                        f"  {group['symbol']} {group['timeframe']}: "
                        f"{group['duplicate_count']}ä»¶ã®é‡è¤‡"
                    )

        elif args.action == "cleanup":
            dry_run = args.dry_run or not args.force

            if not dry_run:
                logger.warning("âš ï¸ å®Ÿéš›ã®å‰Šé™¤ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ã“ã®æ“ä½œã¯å…ƒã«æˆ»ã›ã¾ã›ã‚“ã€‚")
                if not args.force:
                    response = input("ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (yes/no): ")
                    if response.lower() != "yes":
                        logger.info("æ“ä½œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
                        return

            logger.info("ğŸ§¹ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™...")
            cleanup_stats = await cleanup_service.cleanup_duplicates(dry_run=dry_run)

            logger.info(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†:")
            logger.info(
                f"  å‰Šé™¤å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—: {cleanup_stats['total_duplicate_groups']}"
            )
            logger.info(f"  å‰Šé™¤ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {cleanup_stats['total_records_to_delete']}")
            logger.info(f"  ä¿æŒãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {cleanup_stats['records_to_keep']}")

        elif args.action == "verify":
            logger.info("ğŸ” ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœã‚’æ¤œè¨¼ã—ã¦ã„ã¾ã™...")
            verification = await cleanup_service.verify_cleanup()

            if verification["status"] == "clean":
                logger.info("âœ… é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã¯å®Œå…¨ã«å‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
            else:
                logger.warning(
                    f"âš ï¸ {verification['remaining_duplicates']}å€‹ã®é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ãŒæ®‹ã£ã¦ã„ã¾ã™"
                )

        elif args.action == "report":
            logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
            report = await cleanup_service.get_data_quality_report()

            logger.info("ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ:")
            logger.info(f"  ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {report['total_stats']['total_records']:,}")
            logger.info(f"  ã‚·ãƒ³ãƒœãƒ«æ•°: {report['total_stats']['unique_symbols']}")
            logger.info(f"  æ™‚é–“è¶³æ•°: {report['total_stats']['unique_timeframes']}")
            logger.info(
                f"  ãƒ‡ãƒ¼ã‚¿æœŸé–“: {report['total_stats']['earliest_data']} - {report['total_stats']['latest_data']}"
            )

            logger.info("\næ™‚é–“è¶³åˆ¥çµ±è¨ˆ:")
            for tf_stat in report["timeframe_stats"]:
                logger.info(
                    f"  {tf_stat['timeframe']}: {tf_stat['record_count']:,}ãƒ¬ã‚³ãƒ¼ãƒ‰"
                )

            logger.info("\nã‚·ãƒ³ãƒœãƒ«åˆ¥çµ±è¨ˆ:")
            for symbol_stat in report["symbol_stats"]:
                logger.info(
                    f"  {symbol_stat['symbol']}: {symbol_stat['record_count']:,}ãƒ¬ã‚³ãƒ¼ãƒ‰"
                )

        logger.info("âœ… æ“ä½œãŒå®Œäº†ã—ã¾ã—ãŸ")

    except Exception as e:
        logger.error(f"æ“ä½œãŒå¤±æ•—ã—ã¾ã—ãŸ: {e}")
        sys.exit(1)

    finally:
        # æ¥ç¶šã‚’é–‰ã˜ã‚‹
        if "connection_manager" in locals():
            await connection_manager.close()


if __name__ == "__main__":
    asyncio.run(main())
