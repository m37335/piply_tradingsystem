"""
åŠ¹ç‡çš„ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚µãƒ¼ãƒ“ã‚¹

5åˆ†è¶³ãƒ™ãƒ¼ã‚¹ + æ—¥è¶³ã®ã¿å–å¾—ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
APIå‘¼ã³å‡ºã—æ•°ã‚’æœ€å°é™ã«æŠ‘ãˆã¤ã¤ã€ç²¾åº¦ã‚’ä¿ã¤
"""

from datetime import datetime, timedelta
from typing import Dict, List, Optional

import pandas as pd
from sqlalchemy.ext.asyncio import AsyncSession

from src.infrastructure.analysis.pattern_detectors import (
    BreakoutDetector,
    CompositeSignalDetector,
    DivergenceDetector,
    PullbackDetector,
    RSIBattleDetector,
    TrendReversalDetector,
)
from src.infrastructure.database.models.pattern_detection_model import (
    PatternDetectionModel,
)
from src.infrastructure.database.repositories.pattern_detection_repository_impl import (
    PatternDetectionRepositoryImpl,
)
from src.infrastructure.database.repositories.price_data_repository_impl import (
    PriceDataRepositoryImpl,
)
from src.infrastructure.database.repositories.technical_indicator_repository_impl import (
    TechnicalIndicatorRepositoryImpl,
)
from src.infrastructure.database.services.multi_timeframe_technical_indicator_service import (
    MultiTimeframeTechnicalIndicatorService,
)
from src.infrastructure.database.services.timeframe_data_service import (
    TimeframeDataService,
)
from src.utils.logging_config import get_infrastructure_logger

logger = get_infrastructure_logger()


class EfficientPatternDetectionService:
    """
    åŠ¹ç‡çš„ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚µãƒ¼ãƒ“ã‚¹

    ç‰¹å¾´:
    - 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã‚’åŸºæœ¬ã¨ã—ã¦å–å¾—
    - æ—¥è¶³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’åˆ¥é€”å–å¾—ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤æ–­ç”¨ï¼‰
    - H1, H4ã¯5åˆ†è¶³ã‹ã‚‰é›†è¨ˆ
    - APIå‘¼ã³å‡ºã—æ•°ã‚’æœ€å°é™ã«æŠ‘åˆ¶
    """

    def __init__(self, session: AsyncSession):
        """
        åˆæœŸåŒ–

        Args:
            session: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
        """
        self.session = session

        # ãƒªãƒã‚¸ãƒˆãƒªåˆæœŸåŒ–
        self.pattern_repo = PatternDetectionRepositoryImpl(session)
        self.indicator_repo = TechnicalIndicatorRepositoryImpl(session)
        self.price_repo = PriceDataRepositoryImpl(session)

        # æ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
        self.timeframe_service = TimeframeDataService(session)

        # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
        self.technical_indicator_service = MultiTimeframeTechnicalIndicatorService(
            session
        )

        # USD/JPYè¨­å®š
        self.currency_pair = "USD/JPY"

        # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨åˆæœŸåŒ–
        self.detectors = {
            1: TrendReversalDetector(),
            2: PullbackDetector(),
            3: DivergenceDetector(),
            4: BreakoutDetector(),
            5: RSIBattleDetector(),
            6: CompositeSignalDetector(),
        }

        # ãƒ‘ã‚¿ãƒ¼ãƒ³è¨­å®š
        self.pattern_configs = {
            1: {
                "name": "ãƒˆãƒ¬ãƒ³ãƒ‰è»¢æ›",
                "priority": 90,
                "color": "#FF6B6B",
                "description": "ä¸Šä½è¶³ã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰è»¢æ›ã‚·ã‚°ãƒŠãƒ«",
            },
            2: {
                "name": "æŠ¼ã—ç›®ãƒ»æˆ»ã‚Šå£²ã‚Š",
                "priority": 80,
                "color": "#4ECDC4",
                "description": "ãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šä¸­ã®æŠ¼ã—ç›®ãƒ»æˆ»ã‚Šå£²ã‚Š",
            },
            3: {
                "name": "ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹",
                "priority": 85,
                "color": "#45B7D1",
                "description": "ä¾¡æ ¼ã¨æŒ‡æ¨™ã®ä¹–é›¢ã‚·ã‚°ãƒŠãƒ«",
            },
            4: {
                "name": "ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ",
                "priority": 75,
                "color": "#96CEB4",
                "description": "é‡è¦ãªãƒ¬ãƒ™ãƒ«çªç ´ã‚·ã‚°ãƒŠãƒ«",
            },
            5: {
                "name": "RSIãƒãƒˆãƒ«",
                "priority": 70,
                "color": "#FFEAA7",
                "description": "RSIéè²·ã„ãƒ»éå£²ã‚Šã‚¾ãƒ¼ãƒ³ã§ã®æˆ¦ã„",
            },
            6: {
                "name": "è¤‡åˆã‚·ã‚°ãƒŠãƒ«",
                "priority": 95,
                "color": "#DDA0DD",
                "description": "è¤‡æ•°ã®æŒ‡æ¨™ãŒä¸€è‡´ã™ã‚‹å¼·åŠ›ã‚·ã‚°ãƒŠãƒ«",
            },
        }

        logger.info(
            f"Initialized EfficientPatternDetectionService for {self.currency_pair}"
        )

    async def detect_all_patterns(
        self,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> Dict[int, List[PatternDetectionModel]]:
        """
        å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º

        Args:
            start_date: é–‹å§‹æ—¥æ™‚ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: éå»24æ™‚é–“)
            end_date: çµ‚äº†æ—¥æ™‚ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç¾åœ¨æ™‚åˆ»)

        Returns:
            Dict[int, List[PatternDetectionModel]]: ãƒ‘ã‚¿ãƒ¼ãƒ³ç•ªå·åˆ¥ã®æ¤œå‡ºçµæœ
        """
        try:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ—¥æ™‚ã®è¨­å®š
            if end_date is None:
                end_date = datetime.now()
            if start_date is None:
                start_date = end_date - timedelta(hours=24)

            logger.info(f"Detecting all patterns from {start_date} to {end_date}")

            # åŠ¹ç‡çš„ãªãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            multi_timeframe_data = await self._build_efficient_multi_timeframe_data(
                start_date, end_date
            )

            # å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
            all_patterns = {}
            for pattern_number, detector in self.detectors.items():
                try:
                    patterns = await self._detect_single_pattern(
                        pattern_number, detector, multi_timeframe_data
                    )
                    all_patterns[pattern_number] = patterns
                except Exception as e:
                    logger.error(f"âŒ ãƒ‘ã‚¿ãƒ¼ãƒ³{pattern_number}æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    all_patterns[pattern_number] = []

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãã§ä¿å­˜
            for pattern_number, patterns in all_patterns.items():
                if patterns:
                    saved_patterns = await self._save_patterns_with_duplicate_check(
                        patterns
                    )
                    all_patterns[pattern_number] = saved_patterns

            total_patterns = sum(len(patterns) for patterns in all_patterns.values())
            logger.info(f"âœ… å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå®Œäº†: {total_patterns}ä»¶")

            return all_patterns

        except Exception as e:
            logger.error(f"âŒ å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    async def detect_all_patterns_for_timeframe(
        self,
        timeframe: str,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> List[PatternDetectionModel]:
        """
        æŒ‡å®šæ™‚é–“è»¸ã®å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º

        Args:
            timeframe: æ™‚é–“è»¸ï¼ˆ5m, 1h, 4h, 1dï¼‰
            start_date: é–‹å§‹æ—¥æ™‚
            end_date: çµ‚äº†æ—¥æ™‚

        Returns:
            List[PatternDetectionModel]: æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³
        """
        try:
            logger.info(f"ğŸ” {timeframe}æ™‚é–“è»¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºé–‹å§‹")

            # æ—¥ä»˜ç¯„å›²ã®è¨­å®š
            if end_date is None:
                end_date = datetime.now()
            if start_date is None:
                start_date = end_date - timedelta(days=7)

            # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            multi_timeframe_data = await self._build_efficient_multi_timeframe_data(
                start_date, end_date
            )

            # æŒ‡å®šæ™‚é–“è»¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            if timeframe not in multi_timeframe_data:
                logger.warning(f"âš ï¸ {timeframe}æ™‚é–“è»¸ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return []

            timeframe_data = multi_timeframe_data[timeframe]
            if timeframe_data is None or timeframe_data["price_data"].empty:
                logger.warning(f"âš ï¸ {timeframe}æ™‚é–“è»¸ã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                return []

            # å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
            all_patterns = []
            for pattern_number, detector in self.detectors.items():
                try:
                    patterns = await self._detect_single_pattern(
                        pattern_number, detector, multi_timeframe_data
                    )

                    # æŒ‡å®šæ™‚é–“è»¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    timeframe_patterns = [
                        p for p in patterns if p.timeframe == timeframe
                    ]

                    all_patterns.extend(timeframe_patterns)

                except Exception as e:
                    logger.error(f"âŒ ãƒ‘ã‚¿ãƒ¼ãƒ³{pattern_number}æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãã§ä¿å­˜
            saved_patterns = await self._save_patterns_with_duplicate_check(
                all_patterns
            )

            logger.info(
                f"âœ… {timeframe}æ™‚é–“è»¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå®Œäº†: {len(saved_patterns)}ä»¶"
            )
            return saved_patterns

        except Exception as e:
            logger.error(f"âŒ {timeframe}æ™‚é–“è»¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []

    async def detect_patterns_for_timeframe(self, timeframe: str) -> Dict[str, int]:
        """
        æŒ‡å®šæ™‚é–“è»¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚’å®Ÿè¡Œ

        Args:
            timeframe: æ™‚é–“è»¸ï¼ˆ5m, 1h, 4h, 1dï¼‰

        Returns:
            Dict[str, int]: æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
        """
        try:
            logger.info(f"ğŸ” {timeframe}æ™‚é–“è»¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºé–‹å§‹")

            # æ—¢å­˜ã®detect_all_patterns_for_timeframeãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            patterns = await self.detect_all_patterns_for_timeframe(timeframe)

            if patterns:
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                saved_count = 0
                for pattern in patterns:
                    try:
                        await self.pattern_repo.save(pattern)
                        saved_count += 1
                    except Exception as e:
                        logger.error(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

                logger.info(f"âœ… {timeframe}æ™‚é–“è»¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå®Œäº†: {saved_count}ä»¶")
                return {"detected": saved_count}
            else:
                logger.warning(f"âš ï¸ {timeframe}æ™‚é–“è»¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå®Œäº†: 0ä»¶")
                return {"detected": 0}

        except Exception as e:
            logger.error(f"âŒ {timeframe}æ™‚é–“è»¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    async def get_unnotified_patterns(self) -> List[PatternDetectionModel]:
        """
        æœªé€šçŸ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—

        Returns:
            List[PatternDetectionModel]: æœªé€šçŸ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info("ğŸ” æœªé€šçŸ¥ãƒ‘ã‚¿ãƒ¼ãƒ³å–å¾—é–‹å§‹")

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰æœªé€šçŸ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
            unnotified_patterns = await self.pattern_repo.find_unnotified_patterns()

            logger.info(f"âœ… æœªé€šçŸ¥ãƒ‘ã‚¿ãƒ¼ãƒ³å–å¾—å®Œäº†: {len(unnotified_patterns)}ä»¶")
            return unnotified_patterns

        except Exception as e:
            logger.error(f"âŒ æœªé€šçŸ¥ãƒ‘ã‚¿ãƒ¼ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
        try:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ—¥æ™‚ã®è¨­å®š
            if end_date is None:
                end_date = datetime.now()
            if start_date is None:
                start_date = end_date - timedelta(hours=24)

            logger.info(f"Detecting all patterns from {start_date} to {end_date}")

            # åŠ¹ç‡çš„ãªãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            multi_timeframe_data = await self._build_efficient_multi_timeframe_data(
                start_date, end_date
            )

            if not multi_timeframe_data:
                logger.warning(
                    "No multi-timeframe data available for pattern detection"
                )
                return {}

            # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
            results = {}
            for pattern_number, detector in self.detectors.items():
                try:
                    patterns = await self._detect_single_pattern(
                        pattern_number, detector, multi_timeframe_data
                    )
                    if patterns:
                        results[pattern_number] = patterns
                        logger.info(
                            f"Detected {len(patterns)} patterns for pattern {pattern_number}"
                        )

                except Exception as e:
                    logger.error(f"Error detecting pattern {pattern_number}: {e}")
                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
                    logger.debug(f"Pattern {pattern_number} detector: {detector}")
                    logger.debug(
                        f"Multi-timeframe data keys: {list(multi_timeframe_data.keys())}"
                    )
                    for timeframe, data in multi_timeframe_data.items():
                        logger.debug(
                            f"{timeframe} data: price_data={len(data.get('price_data', pd.DataFrame()))}, indicators={list(data.get('indicators', {}).keys())}"
                        )

            logger.info(
                f"Pattern detection completed. Found patterns: {list(results.keys())}"
            )
            return results

        except Exception as e:
            logger.error(f"Error in detect_all_patterns: {e}")
            return {}

    async def _build_efficient_multi_timeframe_data(
        self, start_date: datetime, end_date: datetime
    ) -> Dict:
        """
        åŠ¹ç‡çš„ãªãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰

        ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
        - 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã‚’åŸºæœ¬ã¨ã—ã¦å–å¾—
        - å„æ™‚é–“è»¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’5åˆ†è¶³ã‹ã‚‰å‹•çš„ã«é›†è¨ˆ
        - ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦æŒ‡æ¨™ã‚’å–å¾—
        """
        try:
            # 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆåŸºæœ¬ãƒ‡ãƒ¼ã‚¿ï¼‰- æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºå®Ÿã«å–å¾—
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æœ€æ–°ãƒ‡ãƒ¼ã‚¿æ™‚åˆ»ã‚’å–å¾—
            from sqlalchemy import text

            result = await self.session.execute(
                text(
                    "SELECT MAX(timestamp) as latest_data FROM price_data WHERE currency_pair = :currency_pair"
                ),
                {"currency_pair": self.currency_pair},
            )
            latest_data_str = result.scalar()

            if latest_data_str:
                latest_data = datetime.fromisoformat(
                    latest_data_str.replace("Z", "+00:00")
                )
                # ã‚ˆã‚ŠçŸ­ã„æœŸé–“ã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                actual_start_date = latest_data - timedelta(hours=24)  # 24æ™‚é–“å‰ã‹ã‚‰
                actual_end_date = latest_data
            else:
                actual_start_date = start_date
                actual_end_date = end_date

            m5_price_data = await self.price_repo.find_by_date_range(
                actual_start_date, actual_end_date, self.currency_pair, 1000
            )

            if not m5_price_data:
                logger.warning("No 5m price data available")
                return {}

            # 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
            m5_df = self._convert_to_dataframe(m5_price_data)

            if m5_df.empty:
                logger.warning("5m DataFrame is empty")
                return {}

            # å„æ™‚é–“è»¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’5åˆ†è¶³ã‹ã‚‰é›†è¨ˆ
            h1_df = self._aggregate_timeframe(m5_df, "1H")
            h4_df = self._aggregate_timeframe(m5_df, "4H")
            d1_df = self._aggregate_timeframe(m5_df, "1D")

            # ä¿å­˜æ¸ˆã¿ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆé€²è¡Œä¸­ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰
            saved_h1_data = await self._get_saved_aggregated_data(
                "1h", actual_start_date, actual_end_date
            )
            saved_h4_data = await self._get_saved_aggregated_data(
                "4h", actual_start_date, actual_end_date
            )
            saved_d1_data = await self._get_saved_aggregated_data(
                "1d", actual_start_date, actual_end_date
            )

            # ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆã€ãªã‘ã‚Œã°å‹•çš„é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            final_h1_df = saved_h1_data if not saved_h1_data.empty else h1_df
            final_h4_df = saved_h4_data if not saved_h4_data.empty else h4_df
            final_d1_df = saved_d1_data if not saved_d1_data.empty else d1_df

            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ãƒ­ã‚°å‡ºåŠ›
            if not saved_h1_data.empty:
                logger.info("âœ… 1hæ™‚é–“è»¸: ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
            elif not h1_df.empty:
                logger.info("ğŸ“Š 1hæ™‚é–“è»¸: å‹•çš„é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")

            if not saved_h4_data.empty:
                logger.info("âœ… 4hæ™‚é–“è»¸: ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
            elif not h4_df.empty:
                logger.info("ğŸ“Š 4hæ™‚é–“è»¸: å‹•çš„é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")

            if not saved_d1_data.empty:
                logger.info("âœ… 1dæ™‚é–“è»¸: ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
            elif not d1_df.empty:
                logger.info("ğŸ“Š 1dæ™‚é–“è»¸: å‹•çš„é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")

            # å„æ™‚é–“è»¸ã®æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            m5_indicators = await self.technical_indicator_service.get_latest_indicators_by_timeframe(
                "5m"
            )
            h1_indicators = await self.technical_indicator_service.get_latest_indicators_by_timeframe(
                "1h"
            )
            h4_indicators = await self.technical_indicator_service.get_latest_indicators_by_timeframe(
                "4h"
            )
            d1_indicators = await self.technical_indicator_service.get_latest_indicators_by_timeframe(
                "1d"
            )

            # æŒ‡æ¨™ã‚­ãƒ¼ã‚’çµ±ä¸€ã—ã€ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«å¤‰æ›
            def normalize_indicators(indicators):
                if not indicators:
                    return {}
                normalized = {}

                # RSI: ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«å¤‰æ›
                if "rsi" in indicators:
                    rsi_value = indicators["rsi"]["value"]
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ã¯current_valueã‚’æœŸå¾…
                    normalized["rsi"] = {"current_value": rsi_value}
                    # åŒæ™‚ã«pandas Seriesã‚‚æä¾›ï¼ˆä»–ã®ç”¨é€”ã®ãŸã‚ï¼‰
                    normalized["rsi_series"] = pd.Series([rsi_value] * 20)

                # MACD: ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«å¤‰æ›
                if "macd" in indicators:
                    macd_value = indicators["macd"]["value"]
                    # additional_dataã‹ã‚‰signalã¨histogramã‚’å–å¾—
                    additional_data = indicators["macd"].get("additional_data", {})
                    signal_value = (
                        additional_data.get("signal", [0.0] * 20)[0]
                        if additional_data.get("signal")
                        else 0.0
                    )
                    histogram_value = (
                        additional_data.get("histogram", [0.0] * 20)[0]
                        if additional_data.get("histogram")
                        else 0.0
                    )
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ã¯pandas Seriesã‚’æœŸå¾…
                    normalized["macd"] = {
                        "macd": pd.Series([macd_value] * 20),
                        "signal": pd.Series([signal_value] * 20),
                        "histogram": pd.Series([histogram_value] * 20),
                    }

                # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰: ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«å¤‰æ›
                if "bb" in indicators:
                    bb_value = indicators["bb"]["value"]
                    # additional_dataã‹ã‚‰upperã€middleã€lowerã‚’å–å¾—
                    additional_data = indicators["bb"].get("additional_data", {})
                    upper_value = (
                        additional_data.get("upper", [0.0] * 20)[0]
                        if additional_data.get("upper")
                        else bb_value + 1.0
                    )
                    middle_value = (
                        additional_data.get("middle", [0.0] * 20)[0]
                        if additional_data.get("middle")
                        else bb_value
                    )
                    lower_value = (
                        additional_data.get("lower", [0.0] * 20)[0]
                        if additional_data.get("lower")
                        else bb_value - 1.0
                    )
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ã¯pandas Seriesã‚’æœŸå¾…
                    normalized["bollinger_bands"] = {
                        "upper": pd.Series([upper_value] * 20),
                        "middle": pd.Series([middle_value] * 20),
                        "lower": pd.Series([lower_value] * 20),
                    }

                return normalized

            m5_indicators = normalize_indicators(m5_indicators)
            h1_indicators = normalize_indicators(h1_indicators)
            h4_indicators = normalize_indicators(h4_indicators)
            d1_indicators = normalize_indicators(d1_indicators)

            # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            result_data = {}

            # 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿
            if not m5_df.empty:
                result_data["5m"] = {
                    "price_data": m5_df,
                    "indicators": m5_indicators,
                }

            # 1æ™‚é–“è¶³ãƒ‡ãƒ¼ã‚¿
            if not final_h1_df.empty:
                result_data["1h"] = {
                    "price_data": final_h1_df,
                    "indicators": h1_indicators,
                }

            # 4æ™‚é–“è¶³ãƒ‡ãƒ¼ã‚¿
            if not final_h4_df.empty:
                result_data["4h"] = {
                    "price_data": final_h4_df,
                    "indicators": h4_indicators,
                }

            # æ—¥è¶³ãƒ‡ãƒ¼ã‚¿
            if not final_d1_df.empty:
                result_data["1d"] = {
                    "price_data": final_d1_df,
                    "indicators": d1_indicators,
                }

            logger.info(
                f"Built efficient multi-timeframe data with {len(result_data)} timeframes"
            )
            return result_data

        except Exception as e:
            logger.error(f"Error building efficient multi-timeframe data: {e}")
            return {}

    async def _get_saved_aggregated_data(
        self, timeframe: str, start_date: datetime, end_date: datetime
    ) -> pd.DataFrame:
        """
        ä¿å­˜æ¸ˆã¿ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆé€²è¡Œä¸­ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰

        Args:
            timeframe: æ™‚é–“è»¸ï¼ˆ1h, 4h, 1dï¼‰
            start_date: é–‹å§‹æ—¥æ™‚
            end_date: çµ‚äº†æ—¥æ™‚

        Returns:
            pd.DataFrame: é›†è¨ˆãƒ‡ãƒ¼ã‚¿
        """
        try:
            # æœŸé–“å†…ã®ä¿å­˜æ¸ˆã¿é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢
            saved_data = await self.price_repo.find_by_date_range_and_timeframe(
                start_date, end_date, self.currency_pair, timeframe, 100
            )

            if saved_data:
                # DataFrameã«å¤‰æ›
                df_data = []
                for data in saved_data:
                    # é€²è¡Œä¸­ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯å®Œäº†ãƒ‡ãƒ¼ã‚¿ã®ã„ãšã‚Œã‹
                    if "Aggregated" in data.data_source:
                        df_data.append(
                            {
                                "timestamp": data.timestamp,
                                "Open": float(data.open_price),
                                "High": float(data.high_price),
                                "Low": float(data.low_price),
                                "Close": float(data.close_price),
                                "Volume": int(data.volume),
                            }
                        )

                if df_data:
                    df = pd.DataFrame(df_data)
                    df.set_index("timestamp", inplace=True)
                    df.sort_index(inplace=True)
                    logger.info(f"âœ… {timeframe}ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿å–å¾—: {len(df_data)}ä»¶")
                    return df

            logger.info(f"ğŸ“Š {timeframe}ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãªã—ã€å‹•çš„é›†è¨ˆã‚’ä½¿ç”¨")
            return pd.DataFrame()

        except Exception as e:
            logger.error(f"Error getting saved aggregated data for {timeframe}: {e}")
            return pd.DataFrame()

    def _convert_to_dataframe(self, price_data: List) -> pd.DataFrame:
        """
        ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
        """
        if not price_data:
            return pd.DataFrame()

        df_data = []
        for data in price_data:
            df_data.append(
                {
                    "timestamp": data.timestamp,
                    "Open": float(data.open_price) if data.open_price else 0.0,
                    "High": float(data.high_price) if data.high_price else 0.0,
                    "Low": float(data.low_price) if data.low_price else 0.0,
                    "Close": float(data.close_price) if data.close_price else 0.0,
                    "Volume": int(data.volume) if data.volume else 0,
                }
            )

        df = pd.DataFrame(df_data)
        if not df.empty:
            df.set_index("timestamp", inplace=True)
        return df

    def _aggregate_timeframe(self, df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """
        æ™‚é–“è»¸ã‚’é›†è¨ˆ
        """
        if df.empty:
            return df

        try:
            # OHLCVé›†è¨ˆ
            agg_df = (
                df.resample(timeframe)
                .agg(
                    {
                        "Open": "first",
                        "High": "max",
                        "Low": "min",
                        "Close": "last",
                        "Volume": "sum",
                    }
                )
                .dropna()
            )

            return agg_df

        except Exception as e:
            logger.error(f"Error aggregating timeframe {timeframe}: {e}")
            return pd.DataFrame()

    async def _get_indicators_for_timeframe(
        self, timeframe: str, start_date: datetime, end_date: datetime
    ) -> Dict:
        """
        ç‰¹å®šæ™‚é–“è»¸ã®æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        """
        try:
            indicators = await self.indicator_repo.find_by_date_range(
                start_date, end_date, None, timeframe, self.currency_pair, 100
            )

            indicator_dict = {}

            # RSI
            latest_rsi = await self.indicator_repo.find_latest_by_type(
                "RSI", timeframe, limit=1
            )
            if latest_rsi:
                indicator_dict["rsi"] = {"current_value": float(latest_rsi[0].value)}

            # MACD
            latest_macd = await self.indicator_repo.find_latest_by_type(
                "MACD", timeframe, limit=1
            )
            if latest_macd:
                additional_data = latest_macd[0].additional_data or {}
                indicator_dict["macd"] = {
                    "macd": float(latest_macd[0].value),
                    "signal": additional_data.get("signal_line", 0.0),
                    "histogram": additional_data.get("histogram", 0.0),
                }

            # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
            latest_bb = await self.indicator_repo.find_latest_by_type(
                "BB", timeframe, limit=1
            )
            if latest_bb:
                additional_data = latest_bb[0].additional_data or {}
                indicator_dict["bollinger_bands"] = {
                    "upper": additional_data.get("upper_band", 0.0),
                    "middle": float(latest_bb[0].value),
                    "lower": additional_data.get("lower_band", 0.0),
                }

            return indicator_dict

        except Exception as e:
            logger.error(f"Error getting indicators for timeframe {timeframe}: {e}")
            return {}

    async def _detect_single_pattern(
        self,
        pattern_number: int,
        detector,
        multi_timeframe_data: Dict,
    ) -> List[PatternDetectionModel]:
        """
        å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        """
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ã§æ¤œå‡º
            detection_result = detector.detect(multi_timeframe_data)

            if not detection_result:
                return []

            # æ¤œå‡ºçµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›
            pattern = PatternDetectionModel(
                currency_pair=self.currency_pair,
                timestamp=datetime.now(),
                pattern_type=pattern_number,
                pattern_name=detection_result.get("pattern_name", ""),
                confidence_score=detection_result.get("confidence_score", 0.0),
                direction=(
                    "BUY" if detection_result.get("confidence_score", 0) > 0 else "SELL"
                ),
                detection_data=detection_result.get("conditions_met", {}),
                indicator_data=multi_timeframe_data,
                notification_sent=False,
                notification_sent_at=None,
                notification_message=detection_result.get("notification_title", ""),
            )

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã—ã¦ä¿å­˜
            saved_patterns = await self._save_patterns_with_duplicate_check([pattern])
            return saved_patterns

        except Exception as e:
            logger.error(f"Error in _detect_single_pattern: {e}")
            return []

    async def _save_patterns_with_duplicate_check(
        self, patterns: List[PatternDetectionModel]
    ) -> List[PatternDetectionModel]:
        """
        é‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãã§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿å­˜
        """
        saved_patterns = []

        for pattern in patterns:
            try:
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆéå»1æ™‚é–“ä»¥å†…ã®åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
                existing = await self.pattern_repo.find_recent_duplicate(
                    pattern.currency_pair,
                    pattern.pattern_type,
                    pattern.timestamp,
                    hours=1,
                )

                if existing:
                    logger.info(
                        f"Duplicate pattern detected, skipping: {pattern.pattern_name}"
                    )
                    continue

                # ä¿å­˜
                saved_pattern = await self.pattern_repo.save(pattern)
                saved_patterns.append(saved_pattern)

                logger.info(
                    f"Saved pattern: {pattern.pattern_name} (confidence: {pattern.confidence_score})"
                )

            except Exception as e:
                logger.error(f"Error saving pattern: {e}")

        return saved_patterns

    async def get_latest_patterns(
        self,
        pattern_number: Optional[int] = None,
        limit: int = 10,
    ) -> List[PatternDetectionModel]:
        """
        æœ€æ–°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
        """
        try:
            return await self.pattern_repo.find_latest(
                self.currency_pair, pattern_number, limit
            )
        except Exception as e:
            logger.error(f"Error getting latest patterns: {e}")
            return []

    async def get_pattern_statistics(
        self,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> Dict:
        """
        ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆã‚’å–å¾—
        """
        try:
            if end_date is None:
                end_date = datetime.now()
            if start_date is None:
                start_date = end_date - timedelta(days=7)

            patterns = await self.pattern_repo.find_by_date_range(
                start_date, end_date, self.currency_pair
            )

            stats = {
                "total_patterns": len(patterns),
                "patterns_by_type": {},
                "average_confidence": 0.0,
                "notification_rate": 0.0,
            }

            if patterns:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
                for pattern in patterns:
                    pattern_type = pattern.pattern_type
                    if pattern_type not in stats["patterns_by_type"]:
                        stats["patterns_by_type"][pattern_type] = 0
                    stats["patterns_by_type"][pattern_type] += 1

                # å¹³å‡ä¿¡é ¼åº¦
                total_confidence = sum(p.confidence_score for p in patterns)
                stats["average_confidence"] = total_confidence / len(patterns)

                # é€šçŸ¥ç‡
                notified_count = sum(1 for p in patterns if p.notification_sent)
                stats["notification_rate"] = notified_count / len(patterns)

            return stats

        except Exception as e:
            logger.error(f"Error getting pattern statistics: {e}")
            return {}
