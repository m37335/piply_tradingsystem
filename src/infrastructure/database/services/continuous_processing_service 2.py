"""
ç¶™ç¶šå‡¦ç†çµ±åˆã‚µãƒ¼ãƒ“ã‚¹

è²¬ä»»:
- 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿å–å¾—å¾Œã®è‡ªå‹•é›†è¨ˆå‡¦ç†
- ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—
- ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã®çµ±åˆå®Ÿè¡Œ
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½

ç‰¹å¾´:
- å®Œå…¨è‡ªå‹•åŒ–ã•ã‚ŒãŸç¶™ç¶šå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- å„ã‚¹ãƒ†ãƒƒãƒ—ã®ä¾å­˜é–¢ä¿‚ç®¡ç†
- åŒ…æ‹¬çš„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
"""

import logging
from datetime import datetime
from typing import Any, Dict

from sqlalchemy.ext.asyncio import AsyncSession

from src.infrastructure.database.models.price_data_model import PriceDataModel
from src.infrastructure.database.services.efficient_pattern_detection_service import (
    EfficientPatternDetectionService,
)
from src.infrastructure.database.services.notification_integration_service import (
    NotificationIntegrationService,
)
from src.infrastructure.database.services.talib_technical_indicator_service import (
    TALibTechnicalIndicatorService,
)
from src.infrastructure.database.services.timeframe_aggregator_service import (
    TimeframeAggregatorService,
)

logger = logging.getLogger(__name__)


class ContinuousProcessingService:
    """
    ç¶™ç¶šå‡¦ç†çµ±åˆã‚µãƒ¼ãƒ“ã‚¹

    è²¬ä»»:
    - 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿å–å¾—å¾Œã®è‡ªå‹•é›†è¨ˆå‡¦ç†
    - ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—
    - ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã®çµ±åˆå®Ÿè¡Œ
    - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
    """

    def __init__(self, session: AsyncSession):
        # ä¾å­˜ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
        self.session = session
        self.timeframe_aggregator = TimeframeAggregatorService(session)
        self.technical_indicator_service = TALibTechnicalIndicatorService(session)
        self.pattern_detection_service = EfficientPatternDetectionService(session)
        # é€šçŸ¥ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
        self.notification_service = NotificationIntegrationService(session)

        # è¨­å®š
        self.currency_pair = "USD/JPY"
        self.timeframes = [
            "M5",
            "H1", 
            "H4",
            "D1",
        ]  # TALibTechnicalIndicatorServiceã®å½¢å¼ã«åˆã‚ã›ã‚‹
        self.retry_attempts = 3
        self.retry_delay = 30  # ç§’

        # å‡¦ç†çµ±è¨ˆ
        self.processing_stats = {
            "total_cycles": 0,
            "successful_cycles": 0,
            "failed_cycles": 0,
            "last_processing_time": None,
            "average_processing_time": 0.0,
        }

    async def process_5m_data(self, price_data: PriceDataModel) -> Dict[str, Any]:
        """
        5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã®ç¶™ç¶šå‡¦ç†ã‚’å®Ÿè¡Œ

        Args:
            price_data: å–å¾—ã•ã‚ŒãŸ5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿

        Returns:
            Dict[str, Any]: å‡¦ç†çµæœã®çµ±è¨ˆæƒ…å ±
        """
        start_time = datetime.now()
        self.processing_stats["total_cycles"] += 1

        try:
            logger.info("ğŸ”„ ç¶™ç¶šå‡¦ç†ã‚µã‚¤ã‚¯ãƒ«é–‹å§‹")

            results = {
                "aggregation": {},
                "indicators": {},
                "patterns": {},
                "notifications": {},
                "processing_time": 0,
                "status": "success",
            }

            # 1. 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«DataFetcherServiceã§ä¿å­˜æ¸ˆã¿
            logger.info("ğŸ’¾ 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«ä¿å­˜æ¸ˆã¿ï¼ˆDataFetcherServiceçµŒç”±ï¼‰")
            saved_data = price_data  # æ—¢ã«ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            results["saved_5m_data"] = saved_data

            # 2. æ™‚é–“è»¸é›†è¨ˆã‚’å®Ÿè¡Œ
            logger.info("ğŸ“Š æ™‚é–“è»¸é›†è¨ˆå®Ÿè¡Œä¸­...")
            aggregation_results = await self.aggregate_timeframes()
            results["aggregation"] = aggregation_results

            # 3. ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¨ˆç®—
            logger.info("ğŸ“ˆ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—ä¸­...")
            indicator_results = await self.calculate_all_indicators()
            results["indicators"] = indicator_results

            # 4. ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚’å®Ÿè¡Œ
            logger.info("ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå®Ÿè¡Œä¸­...")
            pattern_results = await self.detect_patterns()
            results["patterns"] = pattern_results

            # 5. é€šçŸ¥å‡¦ç†
            logger.info("ğŸ“¢ é€šçŸ¥å‡¦ç†å®Ÿè¡Œä¸­...")
            notification_results = await self.process_notifications()
            results["notifications"] = notification_results

            # å‡¦ç†æ™‚é–“ã‚’è¨˜éŒ²
            processing_time = (datetime.now() - start_time).total_seconds()
            results["processing_time"] = processing_time
            self.processing_stats["last_processing_time"] = processing_time
            self.processing_stats["successful_cycles"] += 1

            # å¹³å‡å‡¦ç†æ™‚é–“ã‚’æ›´æ–°
            self._update_average_processing_time(processing_time)

            logger.info(f"âœ… ç¶™ç¶šå‡¦ç†ã‚µã‚¤ã‚¯ãƒ«å®Œäº†: {processing_time:.2f}ç§’")
            return results

        except Exception as e:
            self.processing_stats["failed_cycles"] += 1
            processing_time = (datetime.now() - start_time).total_seconds()

            logger.error(f"âŒ ç¶™ç¶šå‡¦ç†ã‚µã‚¤ã‚¯ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "error": str(e),
                "processing_time": processing_time,
                "status": "failed",
            }

    async def aggregate_timeframes(self) -> Dict[str, int]:
        """
        æ™‚é–“è»¸ã®è‡ªå‹•é›†è¨ˆã‚’å®Ÿè¡Œ

        Returns:
            Dict[str, int]: å„æ™‚é–“è»¸ã®é›†è¨ˆä»¶æ•°
        """
        try:
            logger.info("ğŸ“Š æ™‚é–“è»¸é›†è¨ˆé–‹å§‹")

            results = {}

            # 1æ™‚é–“è¶³é›†è¨ˆ
            h1_data = await self.timeframe_aggregator.aggregate_1h_data()
            results["1h"] = len(h1_data)

            # 4æ™‚é–“è¶³é›†è¨ˆ
            h4_data = await self.timeframe_aggregator.aggregate_4h_data()
            results["4h"] = len(h4_data)

            # æ—¥è¶³é›†è¨ˆ
            d1_data = await self.timeframe_aggregator.aggregate_1d_data()
            results["1d"] = len(d1_data)

            logger.info(
                f"âœ… æ™‚é–“è»¸é›†è¨ˆå®Œäº†: 1h={results['1h']}ä»¶, 4h={results['4h']}ä»¶, 1d={results['1d']}ä»¶"
            )
            return results

        except Exception as e:
            logger.error(f"âŒ æ™‚é–“è»¸é›†è¨ˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    async def calculate_all_indicators(self) -> Dict[str, int]:
        """
        å…¨æ™‚é–“è»¸ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¨ˆç®—

        Returns:
            Dict[str, int]: å„æ™‚é–“è»¸ã®æŒ‡æ¨™è¨ˆç®—ä»¶æ•°
        """
        try:
            logger.info("ğŸ“ˆ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—é–‹å§‹")

            results = {}

            # å„æ™‚é–“è»¸ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¨ˆç®—ï¼ˆTA-Libä½¿ç”¨ï¼‰
            for timeframe in self.timeframes:
                indicator_count = await self.technical_indicator_service.calculate_and_save_all_indicators(
                    timeframe
                )
                results[timeframe] = (
                    sum(indicator_count.values())
                    if isinstance(indicator_count, dict)
                    else indicator_count
                )

            logger.info(f"âœ… ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—å®Œäº†: {results}")
            return results

        except Exception as e:
            logger.error(f"âŒ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    async def detect_patterns(self) -> Dict[str, int]:
        """
        ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚’å®Ÿè¡Œ

        Returns:
            Dict[str, int]: æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
        """
        try:
            logger.info("ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºé–‹å§‹")

            results = {}

            # å„æ™‚é–“è»¸ã§ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚’å®Ÿè¡Œ
            for timeframe in self.timeframes:
                pattern_count = (
                    await (
                        self.pattern_detection_service.detect_patterns_for_timeframe(
                            timeframe
                        )
                    )
                )
                results[timeframe] = pattern_count

            logger.info(f"âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå®Œäº†: {results}")
            return results

        except Exception as e:
            logger.error(f"âŒ ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    async def process_notifications(self) -> Dict[str, int]:
        """
        é€šçŸ¥å‡¦ç†ã‚’å®Ÿè¡Œ

        Returns:
            Dict[str, int]: é€ä¿¡ã•ã‚ŒãŸé€šçŸ¥æ•°
        """
        try:
            logger.info("ğŸ“¢ é€šçŸ¥å‡¦ç†é–‹å§‹")

            # æœªé€šçŸ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
            unnotified_patterns = await (
                self.pattern_detection_service.get_unnotified_patterns()
            )

            # é€šçŸ¥ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if self.notification_service is None:
                logger.warning("âš ï¸ é€šçŸ¥ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return {"sent": 0, "total": len(unnotified_patterns), "skipped": True}

            notification_count = 0
            for pattern in unnotified_patterns:
                # é€šçŸ¥ã‚’é€ä¿¡
                success = await self.notification_service.send_pattern_notification(
                    pattern
                )
                if success:
                    # é€šçŸ¥æ¸ˆã¿ãƒ•ãƒ©ã‚°ã‚’æ›´æ–°
                    await self.pattern_detection_service.mark_notification_sent(
                        pattern.id
                    )
                    notification_count += 1

            logger.info(f"âœ… é€šçŸ¥å‡¦ç†å®Œäº†: {notification_count}ä»¶é€ä¿¡")
            return {"sent": notification_count, "total": len(unnotified_patterns)}

        except Exception as e:
            logger.error(f"âŒ é€šçŸ¥å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    async def _save_5m_data(self, price_data: PriceDataModel) -> PriceDataModel:
        """
        5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜

        Args:
            price_data: ä¿å­˜ã™ã‚‹5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿

        Returns:
            PriceDataModel: ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        try:
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: ä¿å­˜å‰ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹
            logger.info(
                f"ğŸ’¾ ä¿å­˜å‰ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹: "
                f"O={price_data.open_price}, H={price_data.high_price}, "
                f"L={price_data.low_price}, C={price_data.close_price}, "
                f"V={price_data.volume}, T={price_data.timestamp}"
            )

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            existing = await self.timeframe_aggregator.price_repo.find_by_timestamp(
                price_data.timestamp, self.currency_pair
            )

            if existing:
                logger.info(f"âš ï¸ 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿é‡è¤‡: {price_data.timestamp}")
                logger.info(
                    f"âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å†…å®¹: "
                    f"O={existing.open_price}, H={existing.high_price}, "
                    f"L={existing.low_price}, C={existing.close_price}, "
                    f"V={existing.volume}"
                )

                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆå‰Šé™¤ã¯ã—ãªã„ï¼‰
                if (
                    existing.open_price
                    == existing.high_price
                    == existing.low_price
                    == existing.close_price
                ):
                    logger.warning(
                        f"âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒåŒã˜OHLCå€¤: {existing.open_price:.4f}"
                    )
                else:
                    logger.info(f"âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¯æ­£å¸¸ãªOHLCå€¤")

                logger.info(f"âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™")
                return existing

            # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            saved_data = await self.timeframe_aggregator.price_repo.save(price_data)
            logger.info(f"ğŸ’¾ 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {price_data.timestamp}")
            logger.info(
                f"ğŸ’¾ ä¿å­˜å¾Œã®ãƒ‡ãƒ¼ã‚¿å†…å®¹: "
                f"O={saved_data.open_price}, H={saved_data.high_price}, "
                f"L={saved_data.low_price}, C={saved_data.close_price}, "
                f"V={saved_data.volume}"
            )
            return saved_data

        except Exception as e:
            logger.error(f"âŒ 5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _update_average_processing_time(self, processing_time: float):
        """
        å¹³å‡å‡¦ç†æ™‚é–“ã‚’æ›´æ–°

        Args:
            processing_time: ä»Šå›ã®å‡¦ç†æ™‚é–“
        """
        if self.processing_stats["successful_cycles"] > 1:
            current_avg = self.processing_stats["average_processing_time"]
            new_avg = (
                current_avg * (self.processing_stats["successful_cycles"] - 1)
                + processing_time
            ) / self.processing_stats["successful_cycles"]
            self.processing_stats["average_processing_time"] = new_avg
        else:
            self.processing_stats["average_processing_time"] = processing_time

    async def get_processing_stats(self) -> Dict[str, Any]:
        """
        å‡¦ç†çµ±è¨ˆã‚’å–å¾—

        Returns:
            Dict[str, Any]: å‡¦ç†çµ±è¨ˆæƒ…å ±
        """
        return {
            **self.processing_stats,
            "success_rate": (
                self.processing_stats["successful_cycles"]
                / max(self.processing_stats["total_cycles"], 1)
                * 100
            ),
            "currency_pair": self.currency_pair,
            "timeframes": self.timeframes,
        }

    async def reset_stats(self):
        """
        å‡¦ç†çµ±è¨ˆã‚’ãƒªã‚»ãƒƒãƒˆ
        """
        self.processing_stats = {
            "total_cycles": 0,
            "successful_cycles": 0,
            "failed_cycles": 0,
            "last_processing_time": None,
            "average_processing_time": 0.0,
        }
        logger.info("ğŸ”„ å‡¦ç†çµ±è¨ˆã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")

    async def process_latest_data(self) -> Dict[str, Any]:
        """
        æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ç¶™ç¶šå‡¦ç†ã‚’å®Ÿè¡Œ

        Returns:
            Dict[str, Any]: å‡¦ç†çµæœ
        """
        try:
            logger.info("ğŸ”„ æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ç¶™ç¶šå‡¦ç†é–‹å§‹")

            # å®Ÿéš›ã®5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            from src.infrastructure.database.services.data_fetcher_service import (
                DataFetcherService,
            )

            data_fetcher = DataFetcherService(self.session)
            latest_data = await data_fetcher.fetch_real_5m_data()

            if not latest_data:
                logger.warning("âš ï¸ å®Ÿéš›ã®5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return {
                    "status": "no_data",
                    "message": "å®Ÿéš›ã®5åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "processing_time": 0,
                }

            # ç¶™ç¶šå‡¦ç†ã‚’å®Ÿè¡Œ
            result = await self.process_5m_data(latest_data)

            logger.info("âœ… æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ç¶™ç¶šå‡¦ç†å®Œäº†")
            return result

        except Exception as e:
            logger.error(f"âŒ æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ç¶™ç¶šå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            self.processing_stats["failed_cycles"] += 1
            return {
                "status": "error",
                "error": str(e),
                "processing_time": 0,
            }

    async def health_check(self) -> Dict[str, Any]:
        """
        ã‚µãƒ¼ãƒ“ã‚¹å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯

        Returns:
            Dict[str, Any]: å¥å…¨æ€§æƒ…å ±
        """
        try:
            health_status = {
                "service": "ContinuousProcessingService",
                "status": "healthy",
                "timestamp": datetime.now(),
                "dependencies": {},
            }

            # ä¾å­˜ã‚µãƒ¼ãƒ“ã‚¹ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
            try:
                await self.timeframe_aggregator.get_aggregation_status()
                health_status["dependencies"]["timeframe_aggregator"] = "healthy"
            except Exception as e:
                health_status["dependencies"][
                    "timeframe_aggregator"
                ] = f"unhealthy: {e}"

            try:
                await self.technical_indicator_service.get_service_status()
                health_status["dependencies"]["technical_indicator_service"] = "healthy"
            except Exception as e:
                health_status["dependencies"][
                    "technical_indicator_service"
                ] = f"unhealthy: {e}"

            try:
                await self.pattern_detection_service.get_service_status()
                health_status["dependencies"]["pattern_detection_service"] = "healthy"
            except Exception as e:
                health_status["dependencies"][
                    "pattern_detection_service"
                ] = f"unhealthy: {e}"

            # å…¨ä½“ã®å¥å…¨æ€§åˆ¤å®š
            unhealthy_deps = [
                dep
                for dep in health_status["dependencies"].values()
                if not dep.startswith("healthy")
            ]

            if unhealthy_deps:
                health_status["status"] = "degraded"
                health_status["issues"] = unhealthy_deps

            return health_status

        except Exception as e:
            return {
                "service": "ContinuousProcessingService",
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now(),
            }
