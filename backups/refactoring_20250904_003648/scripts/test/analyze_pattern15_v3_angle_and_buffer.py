"""
ãƒ‘ã‚¿ãƒ¼ãƒ³15 V3 è§’åº¦åˆ†æã¨ãƒãƒƒãƒ•ã‚¡èª¿æ•´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

è§’åº¦ãŒæ°´å¹³ã«è¿‘ã„ç†ç”±ã®åˆ†æã¨ã€ãƒãƒƒãƒ•ã‚¡ã®ã•ã‚‰ãªã‚‹èª¿æ•´
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict

import numpy as np
import pandas as pd
from scipy.signal import find_peaks
from sqlalchemy import text

from src.infrastructure.analysis.pattern_detectors.support_resistance_detector_v3 import (
    SupportResistanceDetectorV3,
)
from src.infrastructure.database.connection import db_manager

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


class Pattern15V3AngleAndBufferAnalyzer:
    """ãƒ‘ã‚¿ãƒ¼ãƒ³15 V3 è§’åº¦åˆ†æã¨ãƒãƒƒãƒ•ã‚¡èª¿æ•´å™¨"""

    def __init__(self):
        self.timeframes = ["5m", "1h", "1d"]
        self.test_periods = [
            {"name": "3ãƒ¶æœˆ", "days": 90},
            {"name": "6ãƒ¶æœˆ", "days": 180},
            {"name": "1å¹´", "days": 365},
        ]

        # ãƒãƒƒãƒ•ã‚¡èª¿æ•´ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šç´°ã‹ã„èª¿æ•´ï¼‰
        self.buffer_adjustment_patterns = [
            {
                "name": "ç¾åœ¨è¨­å®š",
                "adjustments": {
                    "5m": {
                        "buffer_percentile": 20,
                        "min_peaks": 2,
                        "min_line_strength": 0.4,
                        "max_angle": 45,
                        "price_tolerance": 0.005,
                    },
                    "1h": {
                        "buffer_percentile": 15,
                        "min_peaks": 3,
                        "min_line_strength": 0.6,
                        "max_angle": 30,
                        "price_tolerance": 0.003,
                    },
                    "1d": {
                        "buffer_percentile": 10,
                        "min_peaks": 4,
                        "min_line_strength": 0.8,
                        "max_angle": 20,
                        "price_tolerance": 0.002,
                    },
                },
            },
            {
                "name": "ãƒãƒƒãƒ•ã‚¡æ‹¡å¤§1",
                "adjustments": {
                    "5m": {
                        "buffer_percentile": 35,
                        "min_peaks": 2,
                        "min_line_strength": 0.3,
                        "max_angle": 60,
                        "price_tolerance": 0.008,
                    },
                    "1h": {
                        "buffer_percentile": 30,
                        "min_peaks": 2,
                        "min_line_strength": 0.5,
                        "max_angle": 45,
                        "price_tolerance": 0.005,
                    },
                    "1d": {
                        "buffer_percentile": 25,
                        "min_peaks": 3,
                        "min_line_strength": 0.7,
                        "max_angle": 30,
                        "price_tolerance": 0.003,
                    },
                },
            },
            {
                "name": "ãƒãƒƒãƒ•ã‚¡æ‹¡å¤§2",
                "adjustments": {
                    "5m": {
                        "buffer_percentile": 50,
                        "min_peaks": 1,
                        "min_line_strength": 0.2,
                        "max_angle": 75,
                        "price_tolerance": 0.01,
                    },
                    "1h": {
                        "buffer_percentile": 40,
                        "min_peaks": 2,
                        "min_line_strength": 0.4,
                        "max_angle": 60,
                        "price_tolerance": 0.008,
                    },
                    "1d": {
                        "buffer_percentile": 35,
                        "min_peaks": 2,
                        "min_line_strength": 0.6,
                        "max_angle": 45,
                        "price_tolerance": 0.005,
                    },
                },
            },
            {
                "name": "ãƒãƒƒãƒ•ã‚¡ç¸®å°1",
                "adjustments": {
                    "5m": {
                        "buffer_percentile": 10,
                        "min_peaks": 3,
                        "min_line_strength": 0.5,
                        "max_angle": 30,
                        "price_tolerance": 0.003,
                    },
                    "1h": {
                        "buffer_percentile": 8,
                        "min_peaks": 4,
                        "min_line_strength": 0.7,
                        "max_angle": 20,
                        "price_tolerance": 0.002,
                    },
                    "1d": {
                        "buffer_percentile": 5,
                        "min_peaks": 5,
                        "min_line_strength": 0.9,
                        "max_angle": 15,
                        "price_tolerance": 0.001,
                    },
                },
            },
        ]

    async def analyze_angle_and_buffer(self) -> Dict:
        """è§’åº¦åˆ†æã¨ãƒãƒƒãƒ•ã‚¡èª¿æ•´"""
        logger.info("=== ãƒ‘ã‚¿ãƒ¼ãƒ³15 V3 è§’åº¦åˆ†æã¨ãƒãƒƒãƒ•ã‚¡èª¿æ•´é–‹å§‹ ===")

        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
            await db_manager.initialize(
                "sqlite+aiosqlite:///./data/exchange_analytics.db"
            )
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå®Œäº†")

            # è§’åº¦åˆ†æ
            angle_analysis = await self._analyze_angle_reasons()

            # ãƒãƒƒãƒ•ã‚¡èª¿æ•´ãƒ†ã‚¹ãƒˆ
            buffer_results = await self._test_buffer_adjustments()

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šçµ‚äº†
            await db_manager.close()

            return {
                "angle_analysis": angle_analysis,
                "buffer_results": buffer_results,
                "analysis_time": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"è§’åº¦åˆ†æã¨ãƒãƒƒãƒ•ã‚¡èª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")
            await db_manager.close()
            return {"error": str(e)}

    async def _analyze_angle_reasons(self) -> Dict:
        """è§’åº¦ãŒæ°´å¹³ã«è¿‘ã„ç†ç”±ã®åˆ†æ"""
        try:
            analysis = {}

            # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆ1å¹´åˆ†ï¼‰
            data = await self._fetch_market_data(365)
            if data.empty:
                return {"error": "ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"}

            logger.info(f"è§’åº¦åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿: {len(data)}ä»¶")

            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆ
            high_prices = data["High"].values
            low_prices = data["Low"].values
            close_prices = data["Close"].values

            analysis["price_statistics"] = {
                "high_prices": {
                    "min": float(np.min(high_prices)),
                    "max": float(np.max(high_prices)),
                    "mean": float(np.mean(high_prices)),
                    "std": float(np.std(high_prices)),
                    "range": float(np.max(high_prices) - np.min(high_prices)),
                    "coefficient_of_variation": float(
                        np.std(high_prices) / np.mean(high_prices)
                    ),
                },
                "low_prices": {
                    "min": float(np.min(low_prices)),
                    "max": float(np.max(low_prices)),
                    "mean": float(np.mean(low_prices)),
                    "std": float(np.std(low_prices)),
                    "range": float(np.max(low_prices) - np.min(low_prices)),
                    "coefficient_of_variation": float(
                        np.std(low_prices) / np.mean(low_prices)
                    ),
                },
                "close_prices": {
                    "min": float(np.min(close_prices)),
                    "max": float(np.max(close_prices)),
                    "mean": float(np.mean(close_prices)),
                    "std": float(np.std(close_prices)),
                    "range": float(np.max(close_prices) - np.min(close_prices)),
                    "coefficient_of_variation": float(
                        np.std(close_prices) / np.mean(close_prices)
                    ),
                },
            }

            # ä¾¡æ ¼å¤‰å‹•ã®åˆ†æ
            analysis["price_volatility"] = self._analyze_price_volatility(close_prices)

            # æ¥µå€¤ã®åˆ†æ
            analysis["extremum_analysis"] = self._analyze_extremums(
                high_prices, low_prices
            )

            return analysis

        except Exception as e:
            logger.error(f"è§’åº¦åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _analyze_price_volatility(self, prices: np.ndarray) -> Dict:
        """ä¾¡æ ¼å¤‰å‹•ã®åˆ†æ"""
        try:
            analysis = {}

            # ä¾¡æ ¼å¤‰åŒ–ç‡
            price_changes = np.diff(prices) / prices[:-1]

            analysis["price_changes"] = {
                "mean_change": float(np.mean(price_changes)),
                "std_change": float(np.std(price_changes)),
                "max_change": float(np.max(price_changes)),
                "min_change": float(np.min(price_changes)),
                "abs_mean_change": float(np.mean(np.abs(price_changes))),
            }

            # ãƒ¬ãƒ³ã‚¸åˆ†æ
            rolling_ranges = []
            for i in range(20, len(prices)):
                window = prices[i - 20 : i]
                rolling_ranges.append(np.max(window) - np.min(window))

            analysis["range_analysis"] = {
                "mean_rolling_range": float(np.mean(rolling_ranges)),
                "std_rolling_range": float(np.std(rolling_ranges)),
                "range_stability": float(
                    np.std(rolling_ranges) / np.mean(rolling_ranges)
                ),
            }

            return analysis

        except Exception as e:
            logger.error(f"ä¾¡æ ¼å¤‰å‹•åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _analyze_extremums(
        self, high_prices: np.ndarray, low_prices: np.ndarray
    ) -> Dict:
        """æ¥µå€¤ã®åˆ†æ"""
        try:
            analysis = {}

            # é«˜å€¤ã®æ¥µå¤§å€¤
            high_peaks, _ = find_peaks(high_prices, distance=5)
            if len(high_peaks) > 0:
                peak_prices = high_prices[high_peaks]
                peak_intervals = np.diff(high_peaks)

                analysis["high_peaks"] = {
                    "count": len(high_peaks),
                    "mean_price": float(np.mean(peak_prices)),
                    "std_price": float(np.std(peak_prices)),
                    "price_range": float(np.max(peak_prices) - np.min(peak_prices)),
                    "mean_interval": float(np.mean(peak_intervals)),
                    "std_interval": float(np.std(peak_intervals)),
                }

            # å®‰å€¤ã®æ¥µå°å€¤
            low_peaks, _ = find_peaks(-low_prices, distance=5)
            if len(low_peaks) > 0:
                trough_prices = low_prices[low_peaks]
                trough_intervals = np.diff(low_peaks)

                analysis["low_troughs"] = {
                    "count": len(low_peaks),
                    "mean_price": float(np.mean(trough_prices)),
                    "std_price": float(np.std(trough_prices)),
                    "price_range": float(np.max(trough_prices) - np.min(trough_prices)),
                    "mean_interval": float(np.mean(trough_intervals)),
                    "std_interval": float(np.std(trough_intervals)),
                }

            return analysis

        except Exception as e:
            logger.error(f"æ¥µå€¤åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    async def _test_buffer_adjustments(self) -> Dict:
        """ãƒãƒƒãƒ•ã‚¡èª¿æ•´ã®ãƒ†ã‚¹ãƒˆ"""
        try:
            results = {}

            for pattern in self.buffer_adjustment_patterns:
                logger.info(f"ãƒãƒƒãƒ•ã‚¡èª¿æ•´ãƒ†ã‚¹ãƒˆ: {pattern['name']}")
                pattern_results = {}

                for timeframe in self.timeframes:
                    logger.info(f"  æ™‚é–“è¶³: {timeframe}")
                    timeframe_results = {}

                    for period in self.test_periods:
                        logger.info(f"    æœŸé–“: {period['name']}")
                        result = await self._test_with_buffer_adjustment(
                            timeframe, period, pattern["adjustments"][timeframe]
                        )
                        timeframe_results[period["name"]] = result

                    # æ™‚é–“è¶³åˆ¥çµ±è¨ˆ
                    timeframe_stats = self._analyze_timeframe_statistics(
                        timeframe_results
                    )
                    timeframe_results["statistics"] = timeframe_stats

                    pattern_results[timeframe] = timeframe_results

                # èª¿æ•´ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµ±è¨ˆ
                pattern_stats = self._analyze_pattern_statistics(pattern_results)
                pattern_results["statistics"] = pattern_stats

                results[pattern["name"]] = pattern_results

            return results

        except Exception as e:
            logger.error(f"ãƒãƒƒãƒ•ã‚¡èª¿æ•´ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    async def _test_with_buffer_adjustment(
        self, timeframe: str, period: Dict, adjustments: Dict
    ) -> Dict:
        """ãƒãƒƒãƒ•ã‚¡èª¿æ•´ã§ã®ãƒ†ã‚¹ãƒˆ"""
        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            data = await self._fetch_market_data(period["days"])
            if data.empty:
                return {"error": "ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"}

            logger.info(f"      å–å¾—ãƒ‡ãƒ¼ã‚¿: {len(data)}ä»¶")

            # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ†ã‚¯ã‚¿ãƒ¼ä½œæˆ
            detector = self._create_custom_detector(timeframe, adjustments)

            # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            detection = detector.detect(data)

            if detection:
                # è©³ç´°åˆ†æ
                detailed_analysis = self._analyze_detection_with_angle_details(
                    detection, data, timeframe, period, adjustments
                )
                return {
                    "detected": True,
                    "detection": detection,
                    "analysis": detailed_analysis,
                    "data_points": len(data),
                    "period_days": period["days"],
                    "adjustments": adjustments,
                }
            else:
                return {
                    "detected": False,
                    "data_points": len(data),
                    "period_days": period["days"],
                    "adjustments": adjustments,
                }

        except Exception as e:
            logger.error(f"ãƒãƒƒãƒ•ã‚¡èª¿æ•´ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _create_custom_detector(
        self, timeframe: str, adjustments: Dict
    ) -> SupportResistanceDetectorV3:
        """ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ†ã‚¯ã‚¿ãƒ¼ä½œæˆ"""
        detector = SupportResistanceDetectorV3(timeframe)

        # åŸºæº–å€¤ã‚’èª¿æ•´
        detector.min_peaks = adjustments["min_peaks"]
        detector.buffer_percentile = adjustments["buffer_percentile"]
        detector.min_line_strength = adjustments["min_line_strength"]
        detector.max_angle = adjustments["max_angle"]
        detector.price_tolerance = adjustments["price_tolerance"]

        return detector

    def _analyze_detection_with_angle_details(
        self,
        detection: Dict,
        data: pd.DataFrame,
        timeframe: str,
        period: Dict,
        adjustments: Dict,
    ) -> Dict:
        """è§’åº¦è©³ç´°ã‚’å«ã‚€æ¤œå‡ºåˆ†æ"""
        try:
            analysis = {}

            # åŸºæœ¬æƒ…å ±
            pattern_data = detection.get("pattern_data", {})
            equation = pattern_data.get("equation", {})
            current_analysis = pattern_data.get("current_analysis", {})

            analysis["basic_info"] = {
                "pattern_type": detection.get("pattern_type"),
                "confidence": detection.get("confidence_score"),
                "direction": detection.get("direction"),
                "strategy": detection.get("strategy"),
                "timeframe": timeframe,
                "period": period["name"],
                "adjustments": adjustments,
            }

            # æ•°å­¦çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            analysis["mathematical"] = {
                "slope": equation.get("slope"),
                "intercept": equation.get("intercept"),
                "angle": equation.get("angle"),
                "equation_score": equation.get("score"),
                "angle_description": self._get_angle_description(
                    equation.get("angle", 0)
                ),
            }

            # è§’åº¦ã®è©³ç´°åˆ†æ
            analysis["angle_analysis"] = self._analyze_angle_reasons_detailed(
                equation, data, pattern_data
            )

            # ãƒãƒƒãƒ•ã‚¡åˆ†æ
            analysis["buffer_analysis"] = self._analyze_buffer_effectiveness(
                data, pattern_data, adjustments
            )

            return analysis

        except Exception as e:
            logger.error(f"è§’åº¦è©³ç´°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _analyze_angle_reasons_detailed(
        self, equation: Dict, data: pd.DataFrame, pattern_data: Dict
    ) -> Dict:
        """è§’åº¦ãŒæ°´å¹³ã«è¿‘ã„ç†ç”±ã®è©³ç´°åˆ†æ"""
        try:
            analysis = {}

            angle = equation.get("angle", 0)
            slope = equation.get("slope", 0)

            # è§’åº¦ã®åŸºæœ¬æƒ…å ±
            analysis["angle_basic"] = {
                "angle_degrees": angle,
                "slope": slope,
                "angle_abs": abs(angle),
                "is_horizontal": abs(angle) < 5,
                "is_vertical": abs(angle) > 85,
            }

            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
            high_prices = data["High"].values
            low_prices = data["Low"].values
            close_prices = data["Close"].values

            # ä¾¡æ ¼ã®ä¸€è²«æ€§åˆ†æ
            price_consistency = {
                "high_price_std": float(np.std(high_prices)),
                "low_price_std": float(np.std(low_prices)),
                "close_price_std": float(np.std(close_prices)),
                "price_range": float(np.max(high_prices) - np.min(low_prices)),
                "price_range_ratio": float(
                    (np.max(high_prices) - np.min(low_prices)) / np.mean(close_prices)
                ),
            }

            analysis["price_consistency"] = price_consistency

            # æ¥µå€¤ã®åˆ†æ
            peaks = pattern_data.get("peaks", [])
            troughs = pattern_data.get("troughs", [])

            if peaks:
                peak_prices = [high_prices[i] for i in peaks]
                analysis["peak_analysis"] = {
                    "peak_prices": peak_prices,
                    "peak_price_std": float(np.std(peak_prices)),
                    "peak_price_range": float(
                        np.max(peak_prices) - np.min(peak_prices)
                    ),
                    "peak_price_mean": float(np.mean(peak_prices)),
                }

            if troughs:
                trough_prices = [low_prices[i] for i in troughs]
                analysis["trough_analysis"] = {
                    "trough_prices": trough_prices,
                    "trough_price_std": float(np.std(trough_prices)),
                    "trough_price_range": float(
                        np.max(trough_prices) - np.min(trough_prices)
                    ),
                    "trough_price_mean": float(np.mean(trough_prices)),
                }

            # è§’åº¦ã®ç†ç”±åˆ†æ
            analysis["angle_reasons"] = {
                "price_stability": price_consistency["price_range_ratio"]
                < 0.01,  # ä¾¡æ ¼ãŒå®‰å®šã—ã¦ã„ã‚‹
                "peak_uniformity": len(peaks) > 0
                and analysis.get("peak_analysis", {}).get("peak_price_std", 1)
                < 0.001,  # ãƒ”ãƒ¼ã‚¯ãŒå‡ä¸€
                "trough_uniformity": len(troughs) > 0
                and analysis.get("trough_analysis", {}).get("trough_price_std", 1)
                < 0.001,  # ãƒœãƒˆãƒ ãŒå‡ä¸€
                "timeframe_effect": timeframe in ["1h", "1d"],  # æ™‚é–“è¶³ã®å½±éŸ¿
            }

            return analysis

        except Exception as e:
            logger.error(f"è§’åº¦ç†ç”±è©³ç´°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _analyze_buffer_effectiveness(
        self, data: pd.DataFrame, pattern_data: Dict, adjustments: Dict
    ) -> Dict:
        """ãƒãƒƒãƒ•ã‚¡ã®åŠ¹æœåˆ†æ"""
        try:
            analysis = {}

            buffer_percentile = adjustments["buffer_percentile"]
            high_prices = data["High"].values
            low_prices = data["Low"].values

            # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã®åŠ¹æœ
            high_threshold = np.percentile(high_prices, 100 - buffer_percentile)
            low_threshold = np.percentile(low_prices, buffer_percentile)

            high_buffer_points = np.sum(high_prices >= high_threshold)
            low_buffer_points = np.sum(low_prices <= low_threshold)

            analysis["buffer_effectiveness"] = {
                "buffer_percentile": buffer_percentile,
                "high_threshold": float(high_threshold),
                "low_threshold": float(low_threshold),
                "high_buffer_points": int(high_buffer_points),
                "low_buffer_points": int(low_buffer_points),
                "high_buffer_ratio": float(high_buffer_points / len(high_prices)),
                "low_buffer_ratio": float(low_buffer_points / len(low_prices)),
                "total_buffer_points": int(high_buffer_points + low_buffer_points),
            }

            # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã¨æ¤œå‡ºå“è³ªã®é–¢ä¿‚
            peaks = pattern_data.get("peaks", [])
            troughs = pattern_data.get("troughs", [])

            analysis["detection_quality"] = {
                "peak_count": len(peaks),
                "trough_count": len(troughs),
                "total_extremums": len(peaks) + len(troughs),
                "buffer_efficiency": (
                    float(
                        (len(peaks) + len(troughs))
                        / (high_buffer_points + low_buffer_points)
                    )
                    if (high_buffer_points + low_buffer_points) > 0
                    else 0
                ),
            }

            return analysis

        except Exception as e:
            logger.error(f"ãƒãƒƒãƒ•ã‚¡åŠ¹æœåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _analyze_timeframe_statistics(self, timeframe_results: Dict) -> Dict:
        """æ™‚é–“è¶³åˆ¥çµ±è¨ˆåˆ†æ"""
        try:
            stats = {
                "total_periods": len(
                    [k for k in timeframe_results.keys() if k != "statistics"]
                ),
                "detection_count": 0,
                "detection_rate": 0.0,
                "period_detections": {},
                "confidence_by_period": {},
                "angle_by_period": {},
                "buffer_efficiency_by_period": {},
            }

            for period_name, result in timeframe_results.items():
                if period_name == "statistics":
                    continue

                if result.get("detected", False):
                    stats["detection_count"] += 1
                    stats["period_detections"][period_name] = True

                    # ä¿¡é ¼åº¦çµ±è¨ˆ
                    confidence = result["detection"].get("confidence_score", 0)
                    if period_name not in stats["confidence_by_period"]:
                        stats["confidence_by_period"][period_name] = []
                    stats["confidence_by_period"][period_name].append(confidence)

                    # è§’åº¦çµ±è¨ˆ
                    angle = result["analysis"]["mathematical"]["angle"]
                    if period_name not in stats["angle_by_period"]:
                        stats["angle_by_period"][period_name] = []
                    stats["angle_by_period"][period_name].append(angle)

                    # ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡çµ±è¨ˆ
                    buffer_efficiency = result["analysis"]["buffer_analysis"][
                        "detection_quality"
                    ]["buffer_efficiency"]
                    if period_name not in stats["buffer_efficiency_by_period"]:
                        stats["buffer_efficiency_by_period"][period_name] = []
                    stats["buffer_efficiency_by_period"][period_name].append(
                        buffer_efficiency
                    )
                else:
                    stats["period_detections"][period_name] = False

            # æ¤œå‡ºç‡è¨ˆç®—
            stats["detection_rate"] = stats["detection_count"] / stats["total_periods"]

            # æœŸé–“åˆ¥å¹³å‡å€¤è¨ˆç®—
            for period_name in stats["confidence_by_period"]:
                stats["confidence_by_period"][period_name] = sum(
                    stats["confidence_by_period"][period_name]
                ) / len(stats["confidence_by_period"][period_name])

            for period_name in stats["angle_by_period"]:
                stats["angle_by_period"][period_name] = sum(
                    stats["angle_by_period"][period_name]
                ) / len(stats["angle_by_period"][period_name])

            for period_name in stats["buffer_efficiency_by_period"]:
                stats["buffer_efficiency_by_period"][period_name] = sum(
                    stats["buffer_efficiency_by_period"][period_name]
                ) / len(stats["buffer_efficiency_by_period"][period_name])

            return stats

        except Exception as e:
            logger.error(f"æ™‚é–“è¶³åˆ¥çµ±è¨ˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _analyze_pattern_statistics(self, pattern_results: Dict) -> Dict:
        """èª¿æ•´ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµ±è¨ˆåˆ†æ"""
        try:
            stats = {
                "total_timeframes": len(pattern_results),
                "total_detections": 0,
                "overall_detection_rate": 0.0,
                "timeframe_detection_summary": {},
                "best_performing_timeframe": None,
                "highest_confidence": 0.0,
                "average_angle": 0.0,
                "average_buffer_efficiency": 0.0,
            }

            total_periods = 0
            timeframe_performance = {}
            all_angles = []
            all_buffer_efficiencies = []

            for timeframe, timeframe_results in pattern_results.items():
                if timeframe == "statistics":
                    continue

                timeframe_stats = timeframe_results.get("statistics", {})
                detection_count = timeframe_stats.get("detection_count", 0)
                total_periods_tf = timeframe_stats.get("total_periods", 0)

                stats["total_detections"] += detection_count
                total_periods += total_periods_tf

                detection_rate = timeframe_stats.get("detection_rate", 0.0)
                timeframe_performance[timeframe] = detection_rate

                stats["timeframe_detection_summary"][timeframe] = {
                    "detection_count": detection_count,
                    "total_periods": total_periods_tf,
                    "detection_rate": detection_rate,
                }

                # è§’åº¦ã¨ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡ã®åé›†
                angle_by_period = timeframe_stats.get("angle_by_period", {})
                buffer_efficiency_by_period = timeframe_stats.get(
                    "buffer_efficiency_by_period", {}
                )

                for period_name, angles in angle_by_period.items():
                    all_angles.extend(angles)

                for period_name, efficiencies in buffer_efficiency_by_period.items():
                    all_buffer_efficiencies.extend(efficiencies)

                # æœ€é«˜ä¿¡é ¼åº¦ã®è¿½è·¡
                confidence_by_period = timeframe_stats.get("confidence_by_period", {})
                for period_name, confidences in confidence_by_period.items():
                    if confidences and max(confidences) > stats["highest_confidence"]:
                        stats["highest_confidence"] = max(confidences)

            # å…¨ä½“æ¤œå‡ºç‡
            if total_periods > 0:
                stats["overall_detection_rate"] = (
                    stats["total_detections"] / total_periods
                )

            # æœ€é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ™‚é–“è¶³
            if timeframe_performance:
                stats["best_performing_timeframe"] = max(
                    timeframe_performance, key=timeframe_performance.get
                )

            # å¹³å‡è§’åº¦ã¨ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡
            if all_angles:
                stats["average_angle"] = sum(all_angles) / len(all_angles)
            if all_buffer_efficiencies:
                stats["average_buffer_efficiency"] = sum(all_buffer_efficiencies) / len(
                    all_buffer_efficiencies
                )

            return stats

        except Exception as e:
            logger.error(f"èª¿æ•´ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _get_angle_description(self, angle: float) -> str:
        """è§’åº¦ã®èª¬æ˜ã‚’å–å¾—"""
        abs_angle = abs(angle)
        if abs_angle < 5:
            return "ã»ã¼æ°´å¹³"
        elif abs_angle < 15:
            return "ç·©ã‚„ã‹ãªä¸Šæ˜‡" if angle > 0 else "ç·©ã‚„ã‹ãªä¸‹é™"
        elif abs_angle < 30:
            return "ä¸­ç¨‹åº¦ã®ä¸Šæ˜‡" if angle > 0 else "ä¸­ç¨‹åº¦ã®ä¸‹é™"
        elif abs_angle < 45:
            return "æ€¥ãªä¸Šæ˜‡" if angle > 0 else "æ€¥ãªä¸‹é™"
        else:
            return "éå¸¸ã«æ€¥ãªä¸Šæ˜‡" if angle > 0 else "éå¸¸ã«æ€¥ãªä¸‹é™"

    async def _fetch_market_data(self, days: int) -> pd.DataFrame:
        """å¸‚å ´ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        try:
            async with db_manager.get_session() as session:
                query = text(
                    """
                    SELECT
                        timestamp as Date,
                        open_price as Open,
                        high_price as High,
                        low_price as Low,
                        close_price as Close,
                        volume as Volume
                    FROM price_data
                    WHERE currency_pair = 'USD/JPY'
                    ORDER BY timestamp DESC
                    LIMIT :days
                """
                )

                result = await session.execute(query, {"days": days})
                rows = result.fetchall()

                if not rows:
                    return pd.DataFrame()

                data = pd.DataFrame(
                    rows, columns=["Date", "Open", "High", "Low", "Close", "Volume"]
                )

                data = data.sort_values("Date").reset_index(drop=True)
                return data

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    analyzer = Pattern15V3AngleAndBufferAnalyzer()
    results = await analyzer.analyze_angle_and_buffer()

    if "error" in results:
        print(f"\nâŒ åˆ†æã‚¨ãƒ©ãƒ¼: {results['error']}")
        return

    print("\n=== ãƒ‘ã‚¿ãƒ¼ãƒ³15 V3 è§’åº¦åˆ†æã¨ãƒãƒƒãƒ•ã‚¡èª¿æ•´çµæœ ===")

    # è§’åº¦åˆ†æçµæœ
    angle_analysis = results.get("angle_analysis", {})
    if "error" not in angle_analysis:
        print(f"\nğŸ“ è§’åº¦åˆ†æçµæœ:")

        # ä¾¡æ ¼çµ±è¨ˆ
        price_stats = angle_analysis.get("price_statistics", {})
        if price_stats:
            print(f"  ä¾¡æ ¼çµ±è¨ˆ:")
            high_stats = price_stats.get("high_prices", {})
            print(f"    é«˜å€¤:")
            print(
                f"      ç¯„å›²: {high_stats.get('min', 0):.5f} - {high_stats.get('max', 0):.5f}"
            )
            print(f"      å¤‰å‹•ä¿‚æ•°: {high_stats.get('coefficient_of_variation', 0):.5f}")

            low_stats = price_stats.get("low_prices", {})
            print(f"    å®‰å€¤:")
            print(
                f"      ç¯„å›²: {low_stats.get('min', 0):.5f} - {low_stats.get('max', 0):.5f}"
            )
            print(f"      å¤‰å‹•ä¿‚æ•°: {low_stats.get('coefficient_of_variation', 0):.5f}")

        # ä¾¡æ ¼å¤‰å‹•åˆ†æ
        volatility = angle_analysis.get("price_volatility", {})
        if volatility:
            print(f"  ä¾¡æ ¼å¤‰å‹•åˆ†æ:")
            changes = volatility.get("price_changes", {})
            print(f"    å¹³å‡å¤‰åŒ–ç‡: {changes.get('mean_change', 0):.6f}")
            print(f"    çµ¶å¯¾å¹³å‡å¤‰åŒ–ç‡: {changes.get('abs_mean_change', 0):.6f}")

            range_analysis = volatility.get("range_analysis", {})
            print(f"    ãƒ¬ãƒ³ã‚¸å®‰å®šæ€§: {range_analysis.get('range_stability', 0):.5f}")

        # æ¥µå€¤åˆ†æ
        extremum = angle_analysis.get("extremum_analysis", {})
        if extremum:
            print(f"  æ¥µå€¤åˆ†æ:")
            high_peaks = extremum.get("high_peaks", {})
            if high_peaks:
                print(f"    é«˜å€¤ãƒ”ãƒ¼ã‚¯æ•°: {high_peaks.get('count', 0)}")
                print(f"    é«˜å€¤ãƒ”ãƒ¼ã‚¯ä¾¡æ ¼ç¯„å›²: {high_peaks.get('price_range', 0):.5f}")

            low_troughs = extremum.get("low_troughs", {})
            if low_troughs:
                print(f"    å®‰å€¤ãƒœãƒˆãƒ æ•°: {low_troughs.get('count', 0)}")
                print(f"    å®‰å€¤ãƒœãƒˆãƒ ä¾¡æ ¼ç¯„å›²: {low_troughs.get('price_range', 0):.5f}")

    # ãƒãƒƒãƒ•ã‚¡èª¿æ•´çµæœ
    buffer_results = results.get("buffer_results", {})
    print(f"\nğŸ”§ ãƒãƒƒãƒ•ã‚¡èª¿æ•´çµæœ:")

    for pattern_name, pattern_results in buffer_results.items():
        print(f"\n  {pattern_name}:")

        pattern_stats = pattern_results.get("statistics", {})
        print(f"    ç·æ¤œå‡ºä»¶æ•°: {pattern_stats.get('total_detections', 0)}")
        print(f"    å…¨ä½“æ¤œå‡ºç‡: {pattern_stats.get('overall_detection_rate', 0):.1%}")
        print(f"    æœ€é«˜ä¿¡é ¼åº¦: {pattern_stats.get('highest_confidence', 0):.3f}")
        print(f"    å¹³å‡è§’åº¦: {pattern_stats.get('average_angle', 0):.2f}åº¦")
        print(f"    å¹³å‡ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡: {pattern_stats.get('average_buffer_efficiency', 0):.3f}")

        # æ™‚é–“è¶³åˆ¥çµæœ
        for timeframe, timeframe_data in pattern_results.items():
            if timeframe == "statistics":
                continue

            tf_stats = timeframe_data.get("statistics", {})
            print(
                f"      {timeframe}: {tf_stats.get('detection_count', 0)}ä»¶ ({tf_stats.get('detection_rate', 0):.1%})"
            )

            # è©³ç´°çµæœ
            for period_name, result in timeframe_data.items():
                if period_name == "statistics":
                    continue

                if result.get("detected", False):
                    analysis = result.get("analysis", {})
                    angle_analysis = analysis.get("angle_analysis", {})
                    buffer_analysis = analysis.get("buffer_analysis", {})

                    print(f"        {period_name}:")
                    print(f"          è§’åº¦: {analysis['mathematical']['angle']:.2f}åº¦")
                    print(
                        f"          ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡: {buffer_analysis.get('detection_quality', {}).get('buffer_efficiency', 0):.3f}"
                    )

                    angle_reasons = angle_analysis.get("angle_reasons", {})
                    print(
                        f"          ä¾¡æ ¼å®‰å®šæ€§: {'âœ…' if angle_reasons.get('price_stability', False) else 'âŒ'}"
                    )
                    print(
                        f"          ãƒ”ãƒ¼ã‚¯å‡ä¸€æ€§: {'âœ…' if angle_reasons.get('peak_uniformity', False) else 'âŒ'}"
                    )


if __name__ == "__main__":
    asyncio.run(main())
