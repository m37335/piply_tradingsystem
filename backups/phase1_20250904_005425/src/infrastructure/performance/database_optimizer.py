#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import time
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

from sqlalchemy import inspect, text
from sqlalchemy.ext.asyncio import AsyncSession

from src.infrastructure.config.system_config_manager import SystemConfigManager
from src.infrastructure.discord_webhook_sender import DiscordWebhookSender
from src.infrastructure.monitoring.log_manager import LogManager
from src.utils.logging_config import get_infrastructure_logger

logger = get_infrastructure_logger()


@dataclass
class DatabaseMetrics:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    timestamp: str
    table_sizes: Dict[str, int]
    index_sizes: Dict[str, int]
    query_performance: Dict[str, float]
    connection_count: int
    active_queries: int
    slow_queries: int
    cache_hit_ratio: float
    fragmentation_ratio: float


@dataclass
class OptimizationResult:
    """æœ€é©åŒ–çµæœ"""

    operation: str
    table_name: str
    before_metrics: Dict[str, Any]
    after_metrics: Dict[str, Any]
    improvement: float
    duration: float
    timestamp: str


class DatabaseOptimizer:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
    """

    def __init__(self, config_manager: SystemConfigManager, log_manager: LogManager):
        self.config_manager = config_manager
        self.log_manager = log_manager
        self.optimization_history: List[OptimizationResult] = []
        self.optimization_thresholds = self._load_optimization_thresholds()
        self.optimization_active = False

    def _load_optimization_thresholds(self) -> Dict[str, float]:
        """æœ€é©åŒ–é–¾å€¤ã‚’èª­ã¿è¾¼ã¿"""
        return {
            "table_size_threshold": self.config_manager.get(
                "performance.table_size_threshold", 1000000
            ),
            "fragmentation_threshold": self.config_manager.get(
                "performance.fragmentation_threshold", 30.0
            ),
            "cache_hit_threshold": self.config_manager.get(
                "performance.cache_hit_threshold", 80.0
            ),
            "slow_query_threshold": self.config_manager.get(
                "performance.slow_query_threshold", 5.0
            ),
        }

    async def analyze_database_performance(
        self, session: AsyncSession
    ) -> DatabaseMetrics:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æ"""
        try:
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—
            table_sizes = await self._get_table_sizes(session)

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºã‚’å–å¾—
            index_sizes = await self._get_index_sizes(session)

            # ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å–å¾—
            query_performance = await self._get_query_performance(session)

            # æ¥ç¶šæ•°ã‚’å–å¾—
            connection_count = await self._get_connection_count(session)

            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¯ã‚¨ãƒªæ•°ã‚’å–å¾—
            active_queries = await self._get_active_queries(session)

            # ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªæ•°ã‚’å–å¾—
            slow_queries = await self._get_slow_queries(session)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’å–å¾—
            cache_hit_ratio = await self._get_cache_hit_ratio(session)

            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç‡ã‚’å–å¾—
            fragmentation_ratio = await self._get_fragmentation_ratio(session)

            metrics = DatabaseMetrics(
                timestamp=datetime.now().isoformat(),
                table_sizes=table_sizes,
                index_sizes=index_sizes,
                query_performance=query_performance,
                connection_count=connection_count,
                active_queries=active_queries,
                slow_queries=slow_queries,
                cache_hit_ratio=cache_hit_ratio,
                fragmentation_ratio=fragmentation_ratio,
            )

            return metrics

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return DatabaseMetrics(
                timestamp=datetime.now().isoformat(),
                table_sizes={},
                index_sizes={},
                query_performance={},
                connection_count=0,
                active_queries=0,
                slow_queries=0,
                cache_hit_ratio=0.0,
                fragmentation_ratio=0.0,
            )

    async def _get_table_sizes(self, session: AsyncSession) -> Dict[str, int]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—"""
        try:
            # SQLiteã®å ´åˆ
            result = await session.execute(
                text(
                    """
                SELECT name, sql FROM sqlite_master
                WHERE type='table' AND name NOT LIKE 'sqlite_%'
            """
                )
            )
            tables = result.fetchall()

            table_sizes = {}
            for table_name, _ in tables:
                result = await session.execute(
                    text(f"SELECT COUNT(*) FROM {table_name}")
                )
                count = result.scalar()
                table_sizes[table_name] = count or 0

            return table_sizes

        except Exception as e:
            logger.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºå–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    async def _get_index_sizes(self, session: AsyncSession) -> Dict[str, int]:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºã‚’å–å¾—"""
        try:
            # SQLiteã®å ´åˆ
            result = await session.execute(
                text(
                    """
                SELECT name FROM sqlite_master
                WHERE type='index' AND name NOT LIKE 'sqlite_%'
            """
                )
            )
            indexes = result.fetchall()

            index_sizes = {}
            for (index_name,) in indexes:
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚µã‚¤ã‚ºã‚’æ¨å®š
                index_sizes[index_name] = 1000  # æ¨å®šå€¤

            return index_sizes

        except Exception as e:
            logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºå–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    async def _get_query_performance(self, session: AsyncSession) -> Dict[str, float]:
        """ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å–å¾—"""
        try:
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚¯ã‚¨ãƒªãƒ­ã‚°ã‹ã‚‰çµ±è¨ˆã‚’å–å¾—
            # ç¾åœ¨ã¯æ¨å®šå€¤ã‚’ä½¿ç”¨
            return {
                "data_fetch": 0.5,
                "pattern_detection": 1.2,
                "indicator_calculation": 0.8,
            }

        except Exception as e:
            logger.error(f"ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    async def _get_connection_count(self, session: AsyncSession) -> int:
        """æ¥ç¶šæ•°ã‚’å–å¾—"""
        try:
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‹ã‚‰å–å¾—
            return 1  # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°

        except Exception as e:
            logger.error(f"æ¥ç¶šæ•°å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    async def _get_active_queries(self, session: AsyncSession) -> int:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¯ã‚¨ãƒªæ•°ã‚’å–å¾—"""
        try:
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¯ã‚¨ãƒªã‚’ç›£è¦–
            return 0

        except Exception as e:
            logger.error(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¯ã‚¨ãƒªæ•°å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    async def _get_slow_queries(self, session: AsyncSession) -> int:
        """ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªæ•°ã‚’å–å¾—"""
        try:
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªãƒ­ã‚°ã‹ã‚‰å–å¾—
            return 0

        except Exception as e:
            logger.error(f"ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªæ•°å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    async def _get_cache_hit_ratio(self, session: AsyncSession) -> float:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’å–å¾—"""
        try:
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‹ã‚‰å–å¾—
            return 85.0  # æ¨å®šå€¤

        except Exception as e:
            logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    async def _get_fragmentation_ratio(self, session: AsyncSession) -> float:
        """ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç‡ã‚’å–å¾—"""
        try:
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã‹ã‚‰å–å¾—
            return 15.0  # æ¨å®šå€¤

        except Exception as e:
            logger.error(f"ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç‡å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    async def optimize_database(
        self, session: AsyncSession
    ) -> List[OptimizationResult]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æœ€é©åŒ–"""
        try:
            logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚’é–‹å§‹")

            # æœ€é©åŒ–å‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
            before_metrics = await self.analyze_database_performance(session)

            optimization_results = []

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–
            index_result = await self._optimize_indexes(session, before_metrics)
            if index_result:
                optimization_results.append(index_result)

            # ãƒ†ãƒ¼ãƒ–ãƒ«æœ€é©åŒ–
            table_result = await self._optimize_tables(session, before_metrics)
            if table_result:
                optimization_results.append(table_result)

            # ã‚¯ã‚¨ãƒªæœ€é©åŒ–
            query_result = await self._optimize_queries(session, before_metrics)
            if query_result:
                optimization_results.append(query_result)

            # æœ€é©åŒ–å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
            after_metrics = await self.analyze_database_performance(session)

            # æœ€é©åŒ–çµæœã‚’è¨˜éŒ²
            for result in optimization_results:
                self.optimization_history.append(result)

            # æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’Discordã«é€ä¿¡
            await self._send_optimization_report_to_discord(
                before_metrics, after_metrics, optimization_results
            )

            logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†: {len(optimization_results)}ä»¶ã®æœ€é©åŒ–ã‚’å®Ÿè¡Œ")
            return optimization_results

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    async def _optimize_indexes(
        self, session: AsyncSession, metrics: DatabaseMetrics
    ) -> Optional[OptimizationResult]:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æœ€é©åŒ–"""
        try:
            start_time = time.time()

            # æœ€é©åŒ–ãŒå¿…è¦ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç‰¹å®š
            optimization_needed = []

            for table_name, size in metrics.table_sizes.items():
                if size > self.optimization_thresholds["table_size_threshold"]:
                    # å¤§ããªãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå¿…è¦
                    optimization_needed.append(table_name)

            if not optimization_needed:
                return None

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
            for table_name in optimization_needed:
                # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                result = await session.execute(
                    text(
                        f"""
                    SELECT name FROM sqlite_master
                    WHERE type='index' AND tbl_name='{table_name}'
                """
                    )
                )
                existing_indexes = [row[0] for row in result.fetchall()]

                # å¿…è¦ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
                if f"idx_{table_name}_timestamp" not in existing_indexes:
                    await session.execute(
                        text(
                            f"""
                        CREATE INDEX IF NOT EXISTS idx_{table_name}_timestamp
                        ON {table_name} (timestamp)
                    """
                        )
                    )

                if f"idx_{table_name}_created_at" not in existing_indexes:
                    await session.execute(
                        text(
                            f"""
                        CREATE INDEX IF NOT EXISTS idx_{table_name}_created_at
                        ON {table_name} (created_at)
                    """
                        )
                    )

            await session.commit()

            duration = time.time() - start_time

            # æœ€é©åŒ–å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
            after_metrics = await self.analyze_database_performance(session)

            # æ”¹å–„åº¦ã‚’è¨ˆç®—
            improvement = self._calculate_improvement(
                metrics.query_performance, after_metrics.query_performance
            )

            return OptimizationResult(
                operation="index_optimization",
                table_name="all_tables",
                before_metrics=asdict(metrics),
                after_metrics=asdict(after_metrics),
                improvement=improvement,
                duration=duration,
                timestamp=datetime.now().isoformat(),
            )

        except Exception as e:
            logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    async def _optimize_tables(
        self, session: AsyncSession, metrics: DatabaseMetrics
    ) -> Optional[OptimizationResult]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æœ€é©åŒ–"""
        try:
            start_time = time.time()

            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒé«˜ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æœ€é©åŒ–
            if (
                metrics.fragmentation_ratio
                > self.optimization_thresholds["fragmentation_threshold"]
            ):
                # VACUUMã‚’å®Ÿè¡Œï¼ˆSQLiteã®å ´åˆï¼‰
                await session.execute(text("VACUUM"))
                await session.commit()

                duration = time.time() - start_time

                # æœ€é©åŒ–å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
                after_metrics = await self.analyze_database_performance(session)

                # æ”¹å–„åº¦ã‚’è¨ˆç®—
                improvement = (
                    metrics.fragmentation_ratio - after_metrics.fragmentation_ratio
                )

                return OptimizationResult(
                    operation="table_optimization",
                    table_name="all_tables",
                    before_metrics=asdict(metrics),
                    after_metrics=asdict(after_metrics),
                    improvement=improvement,
                    duration=duration,
                    timestamp=datetime.now().isoformat(),
                )

            return None

        except Exception as e:
            logger.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«æœ€é©åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    async def _optimize_queries(
        self, session: AsyncSession, metrics: DatabaseMetrics
    ) -> Optional[OptimizationResult]:
        """ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–"""
        try:
            start_time = time.time()

            # ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªãŒå­˜åœ¨ã™ã‚‹å ´åˆã®æœ€é©åŒ–
            if metrics.slow_queries > 0:
                # ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
                await session.execute(text("PRAGMA optimize"))
                await session.commit()

                duration = time.time() - start_time

                # æœ€é©åŒ–å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
                after_metrics = await self.analyze_database_performance(session)

                # æ”¹å–„åº¦ã‚’è¨ˆç®—
                improvement = metrics.slow_queries - after_metrics.slow_queries

                return OptimizationResult(
                    operation="query_optimization",
                    table_name="all_tables",
                    before_metrics=asdict(metrics),
                    after_metrics=asdict(after_metrics),
                    improvement=improvement,
                    duration=duration,
                    timestamp=datetime.now().isoformat(),
                )

            return None

        except Exception as e:
            logger.error(f"ã‚¯ã‚¨ãƒªæœ€é©åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _calculate_improvement(
        self, before: Dict[str, float], after: Dict[str, float]
    ) -> float:
        """æ”¹å–„åº¦ã‚’è¨ˆç®—"""
        try:
            if not before or not after:
                return 0.0

            total_before = sum(before.values())
            total_after = sum(after.values())

            if total_before == 0:
                return 0.0

            return ((total_before - total_after) / total_before) * 100

        except Exception:
            return 0.0

    async def _send_optimization_report_to_discord(
        self,
        before_metrics: DatabaseMetrics,
        after_metrics: DatabaseMetrics,
        results: List[OptimizationResult],
    ):
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’Discordã«é€ä¿¡"""
        try:
            webhook_url = self.config_manager.get(
                "notifications.discord_monitoring.webhook_url"
            )
            if not webhook_url:
                webhook_url = self.config_manager.get(
                    "notifications.discord.webhook_url"
                )

            if webhook_url and results:
                async with DiscordWebhookSender(webhook_url) as sender:
                    # æ”¹å–„åº¦ã‚’è¨ˆç®—
                    total_improvement = sum(result.improvement for result in results)
                    total_duration = sum(result.duration for result in results)

                    embed = {
                        "title": "ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†",
                        "description": f"æœ€é©åŒ–ä»¶æ•°: {len(results)}ä»¶\n"
                        f"ç·æ”¹å–„åº¦: {total_improvement:.1f}%\n"
                        f"å®Ÿè¡Œæ™‚é–“: {total_duration:.2f}ç§’",
                        "color": 0x00FF00,
                        "timestamp": datetime.now().isoformat(),
                        "fields": [],
                    }

                    # å„æœ€é©åŒ–çµæœã‚’è¿½åŠ 
                    for result in results:
                        embed["fields"].append(
                            {
                                "name": f"æœ€é©åŒ–: {result.operation}",
                                "value": f"æ”¹å–„åº¦: {result.improvement:.1f}%\n"
                                f"å®Ÿè¡Œæ™‚é–“: {result.duration:.2f}ç§’",
                                "inline": True,
                            }
                        )

                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ
                    embed["fields"].append(
                        {
                            "name": "æœ€é©åŒ–å‰å¾Œæ¯”è¼ƒ",
                            "value": f"ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒª: {before_metrics.slow_queries} â†’ {after_metrics.slow_queries}\n"
                            f"ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {before_metrics.fragmentation_ratio:.1f}% â†’ {after_metrics.fragmentation_ratio:.1f}%",
                            "inline": False,
                        }
                    )

                    await sender.send_embed(embed)
                    logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’Discordã«é€ä¿¡")

        except Exception as e:
            logger.error(f"æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã®Discordé€ä¿¡ã«å¤±æ•—: {e}")

    async def get_optimization_history(self, days: int = 7) -> List[OptimizationResult]:
        """æœ€é©åŒ–å±¥æ­´ã‚’å–å¾—"""
        try:
            cutoff_time = datetime.now() - timedelta(days=days)

            recent_results = [
                result
                for result in self.optimization_history
                if datetime.fromisoformat(result.timestamp) > cutoff_time
            ]

            return recent_results

        except Exception as e:
            logger.error(f"æœ€é©åŒ–å±¥æ­´å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    async def schedule_optimization(
        self, session: AsyncSession, interval_hours: int = 24
    ):
        """å®šæœŸçš„ãªæœ€é©åŒ–ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"""
        try:
            while True:
                logger.info(f"å®šæœŸçš„ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚’å®Ÿè¡Œ (é–“éš”: {interval_hours}æ™‚é–“)")

                await self.optimize_database(session)

                # æŒ‡å®šã•ã‚ŒãŸé–“éš”ã§å¾…æ©Ÿ
                await asyncio.sleep(interval_hours * 3600)

        except Exception as e:
            logger.error(f"å®šæœŸçš„ãªæœ€é©åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    def clear_optimization_history(self):
        """æœ€é©åŒ–å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        self.optimization_history.clear()
        logger.info("æœ€é©åŒ–å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
