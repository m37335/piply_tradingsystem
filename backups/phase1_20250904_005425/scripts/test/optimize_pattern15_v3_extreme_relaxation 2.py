"""
ãƒ‘ã‚¿ãƒ¼ãƒ³15 V3 æ¥µé™ç·©å’Œãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æœˆ10-20å›ã®æ¤œå‡ºã‚’ç›®æŒ‡ã—ãŸæ¥µé™ç·©å’Œè¨­å®š
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict

import numpy as np
import pandas as pd
from sqlalchemy import text

from src.infrastructure.analysis.pattern_detectors.support_resistance_detector_v3 import (
    SupportResistanceDetectorV3,
)
from src.infrastructure.database.connection import db_manager

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


class Pattern15V3ExtremeRelaxationTester:
    """ãƒ‘ã‚¿ãƒ¼ãƒ³15 V3 æ¥µé™ç·©å’Œãƒ†ã‚¹ã‚¿ãƒ¼"""

    def __init__(self):
        self.timeframes = ["5m", "1h", "1d"]
        self.test_periods = [
            {"name": "1ãƒ¶æœˆ", "days": 30},
            {"name": "3ãƒ¶æœˆ", "days": 90},
            {"name": "6ãƒ¶æœˆ", "days": 180},
            {"name": "1å¹´", "days": 365},
        ]
        
        # æ¥µé™ç·©å’Œè¨­å®šãƒ‘ã‚¿ãƒ¼ãƒ³
        self.extreme_relaxation_patterns = [
            {
                "name": "æ¥µé™ç·©å’Œ1ï¼ˆ50å€ç·©å’Œï¼‰",
                "settings": {
                    "5m": {
                        "buffer_percentile": 50,
                        "min_peaks": 1,
                        "min_line_strength": 0.05,
                        "max_angle": 90,
                        "price_tolerance": 0.02,
                    },
                    "1h": {
                        "buffer_percentile": 40,
                        "min_peaks": 1,
                        "min_line_strength": 0.1,
                        "max_angle": 75,
                        "price_tolerance": 0.015,
                    },
                    "1d": {
                        "buffer_percentile": 30,
                        "min_peaks": 1,
                        "min_line_strength": 0.2,
                        "max_angle": 60,
                        "price_tolerance": 0.01,
                    },
                },
            },
            {
                "name": "æ¥µé™ç·©å’Œ2ï¼ˆ100å€ç·©å’Œï¼‰",
                "settings": {
                    "5m": {
                        "buffer_percentile": 70,
                        "min_peaks": 1,
                        "min_line_strength": 0.02,
                        "max_angle": 90,
                        "price_tolerance": 0.03,
                    },
                    "1h": {
                        "buffer_percentile": 60,
                        "min_peaks": 1,
                        "min_line_strength": 0.05,
                        "max_angle": 90,
                        "price_tolerance": 0.025,
                    },
                    "1d": {
                        "buffer_percentile": 50,
                        "min_peaks": 1,
                        "min_line_strength": 0.1,
                        "max_angle": 75,
                        "price_tolerance": 0.02,
                    },
                },
            },
            {
                "name": "æ¥µé™ç·©å’Œ3ï¼ˆ150å€ç·©å’Œï¼‰",
                "settings": {
                    "5m": {
                        "buffer_percentile": 80,
                        "min_peaks": 1,
                        "min_line_strength": 0.01,
                        "max_angle": 90,
                        "price_tolerance": 0.05,
                    },
                    "1h": {
                        "buffer_percentile": 75,
                        "min_peaks": 1,
                        "min_line_strength": 0.02,
                        "max_angle": 90,
                        "price_tolerance": 0.04,
                    },
                    "1d": {
                        "buffer_percentile": 70,
                        "min_peaks": 1,
                        "min_line_strength": 0.05,
                        "max_angle": 90,
                        "price_tolerance": 0.03,
                    },
                },
            },
            {
                "name": "æ¥µé™ç·©å’Œ4ï¼ˆ200å€ç·©å’Œï¼‰",
                "settings": {
                    "5m": {
                        "buffer_percentile": 90,
                        "min_peaks": 1,
                        "min_line_strength": 0.005,
                        "max_angle": 90,
                        "price_tolerance": 0.08,
                    },
                    "1h": {
                        "buffer_percentile": 85,
                        "min_peaks": 1,
                        "min_line_strength": 0.01,
                        "max_angle": 90,
                        "price_tolerance": 0.06,
                    },
                    "1d": {
                        "buffer_percentile": 80,
                        "min_peaks": 1,
                        "min_line_strength": 0.02,
                        "max_angle": 90,
                        "price_tolerance": 0.05,
                    },
                },
            },
        ]

    async def test_extreme_relaxation(self) -> Dict:
        """æ¥µé™ç·©å’Œã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ‘ã‚¿ãƒ¼ãƒ³15 V3 æ¥µé™ç·©å’Œãƒ†ã‚¹ãƒˆé–‹å§‹ ===")

        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
            await db_manager.initialize(
                "sqlite+aiosqlite:///./data/exchange_analytics.db"
            )
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå®Œäº†")

            # å„æ¥µé™ç·©å’Œè¨­å®šã§ã®ãƒ†ã‚¹ãƒˆ
            relaxation_results = await self._test_all_extreme_patterns()

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šçµ‚äº†
            await db_manager.close()

            return {
                "relaxation_results": relaxation_results,
                "analysis_time": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"æ¥µé™ç·©å’Œãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            await db_manager.close()
            return {"error": str(e)}

    async def _test_all_extreme_patterns(self) -> Dict:
        """å…¨æ¥µé™ç·©å’Œè¨­å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
        try:
            results = {}

            for pattern in self.extreme_relaxation_patterns:
                logger.info(f"æ¥µé™ç·©å’Œè¨­å®šãƒ†ã‚¹ãƒˆ: {pattern['name']}")
                pattern_results = {}

                for timeframe in self.timeframes:
                    logger.info(f"  æ™‚é–“è¶³: {timeframe}")
                    timeframe_results = {}

                    for period in self.test_periods:
                        logger.info(f"    æœŸé–“: {period['name']}")
                        result = await self._test_with_extreme_settings(
                            timeframe, period, pattern["settings"][timeframe]
                        )
                        timeframe_results[period["name"]] = result

                    # æ™‚é–“è¶³åˆ¥çµ±è¨ˆ
                    timeframe_stats = self._analyze_timeframe_statistics(timeframe_results)
                    timeframe_results["statistics"] = timeframe_stats

                    pattern_results[timeframe] = timeframe_results

                # æ¥µé™ç·©å’Œãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµ±è¨ˆ
                pattern_stats = self._analyze_pattern_statistics(pattern_results)
                pattern_results["statistics"] = pattern_stats

                results[pattern["name"]] = pattern_results

            return results

        except Exception as e:
            logger.error(f"å…¨æ¥µé™ç·©å’Œè¨­å®šãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    async def _test_with_extreme_settings(
        self, timeframe: str, period: Dict, settings: Dict
    ) -> Dict:
        """æ¥µé™ç·©å’Œè¨­å®šã§ã®ãƒ†ã‚¹ãƒˆ"""
        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            data = await self._fetch_market_data(period["days"])
            if data.empty:
                return {"error": "ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"}

            logger.info(f"      å–å¾—ãƒ‡ãƒ¼ã‚¿: {len(data)}ä»¶")

            # æ¥µé™ç·©å’Œã•ã‚ŒãŸãƒ‡ãƒ†ã‚¯ã‚¿ãƒ¼ä½œæˆ
            detector = self._create_extreme_relaxed_detector(timeframe, settings)

            # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            detection = detector.detect(data)

            if detection:
                # è©³ç´°åˆ†æ
                detailed_analysis = self._analyze_detection_with_details(
                    detection, data, timeframe, period, settings
                )
                return {
                    "detected": True,
                    "detection": detection,
                    "analysis": detailed_analysis,
                    "data_points": len(data),
                    "period_days": period["days"],
                    "settings": settings,
                }
            else:
                return {
                    "detected": False,
                    "data_points": len(data),
                    "period_days": period["days"],
                    "settings": settings,
                }

        except Exception as e:
            logger.error(f"æ¥µé™ç·©å’Œè¨­å®šãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _create_extreme_relaxed_detector(self, timeframe: str, settings: Dict) -> SupportResistanceDetectorV3:
        """æ¥µé™ç·©å’Œã•ã‚ŒãŸãƒ‡ãƒ†ã‚¯ã‚¿ãƒ¼ä½œæˆ"""
        detector = SupportResistanceDetectorV3(timeframe)
        
        # æ¥µé™ç·©å’Œè¨­å®šã‚’é©ç”¨
        detector.min_peaks = settings["min_peaks"]
        detector.buffer_percentile = settings["buffer_percentile"]
        detector.min_line_strength = settings["min_line_strength"]
        detector.max_angle = settings["max_angle"]
        detector.price_tolerance = settings["price_tolerance"]
        
        return detector

    def _analyze_detection_with_details(
        self, detection: Dict, data: pd.DataFrame, timeframe: str, period: Dict, settings: Dict
    ) -> Dict:
        """æ¤œå‡ºè©³ç´°åˆ†æ"""
        try:
            analysis = {}

            # åŸºæœ¬æƒ…å ±
            pattern_data = detection.get("pattern_data", {})
            equation = pattern_data.get("equation", {})

            analysis["basic_info"] = {
                "pattern_type": detection.get("pattern_type"),
                "confidence": detection.get("confidence_score"),
                "direction": detection.get("direction"),
                "strategy": detection.get("strategy"),
                "timeframe": timeframe,
                "period": period["name"],
                "settings": settings,
            }

            # æ•°å­¦çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            slope = equation.get("slope", 0)
            angle = equation.get("angle", 0)
            
            analysis["mathematical"] = {
                "slope": slope,
                "intercept": equation.get("intercept"),
                "angle": angle,
                "equation_score": equation.get("score"),
                "slope_description": self._get_slope_description(slope),
                "angle_description": self._get_angle_description(angle),
            }

            # æ¤œå‡ºå“è³ªåˆ†æ
            analysis["detection_quality"] = self._analyze_detection_quality(
                data, pattern_data, settings
            )

            return analysis

        except Exception as e:
            logger.error(f"æ¤œå‡ºè©³ç´°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _analyze_detection_quality(
        self, data: pd.DataFrame, pattern_data: Dict, settings: Dict
    ) -> Dict:
        """æ¤œå‡ºå“è³ªåˆ†æ"""
        try:
            analysis = {}

            # ãƒãƒƒãƒ•ã‚¡åŠ¹æœåˆ†æ
            buffer_percentile = settings["buffer_percentile"]
            high_prices = data["High"].values
            low_prices = data["Low"].values

            high_threshold = np.percentile(high_prices, 100 - buffer_percentile)
            low_threshold = np.percentile(low_prices, buffer_percentile)

            high_buffer_points = np.sum(high_prices >= high_threshold)
            low_buffer_points = np.sum(low_prices <= low_threshold)

            analysis["buffer_effectiveness"] = {
                "buffer_percentile": buffer_percentile,
                "high_threshold": float(high_threshold),
                "low_threshold": float(low_threshold),
                "high_buffer_points": int(high_buffer_points),
                "low_buffer_points": int(low_buffer_points),
                "total_buffer_points": int(high_buffer_points + low_buffer_points),
                "buffer_coverage": float((high_buffer_points + low_buffer_points) / len(high_prices)),
            }

            # æ¤œå‡ºå“è³ª
            peaks = pattern_data.get("peaks", [])
            troughs = pattern_data.get("troughs", [])
            
            analysis["detection_metrics"] = {
                "peak_count": len(peaks),
                "trough_count": len(troughs),
                "total_extremums": len(peaks) + len(troughs),
                "buffer_efficiency": (
                    float((len(peaks) + len(troughs)) / (high_buffer_points + low_buffer_points))
                    if (high_buffer_points + low_buffer_points) > 0
                    else 0
                ),
                "line_strength": pattern_data.get("strength", 0),
            }

            return analysis

        except Exception as e:
            logger.error(f"æ¤œå‡ºå“è³ªåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _analyze_timeframe_statistics(self, timeframe_results: Dict) -> Dict:
        """æ™‚é–“è¶³åˆ¥çµ±è¨ˆåˆ†æ"""
        try:
            stats = {
                "total_periods": len(
                    [k for k in timeframe_results.keys() if k != "statistics"]
                ),
                "detection_count": 0,
                "detection_rate": 0.0,
                "period_detections": {},
                "confidence_by_period": {},
                "slope_by_period": {},
                "angle_by_period": {},
                "buffer_efficiency_by_period": {},
                "total_buffer_points": 0,
            }

            for period_name, result in timeframe_results.items():
                if period_name == "statistics":
                    continue

                if result.get("detected", False):
                    stats["detection_count"] += 1
                    stats["period_detections"][period_name] = True

                    # ä¿¡é ¼åº¦çµ±è¨ˆ
                    confidence = result["detection"].get("confidence_score", 0)
                    if period_name not in stats["confidence_by_period"]:
                        stats["confidence_by_period"][period_name] = []
                    stats["confidence_by_period"][period_name].append(confidence)

                    # å‚¾ãçµ±è¨ˆ
                    slope = result["analysis"]["mathematical"]["slope"]
                    if period_name not in stats["slope_by_period"]:
                        stats["slope_by_period"][period_name] = []
                    stats["slope_by_period"][period_name].append(slope)

                    # è§’åº¦çµ±è¨ˆ
                    angle = result["analysis"]["mathematical"]["angle"]
                    if period_name not in stats["angle_by_period"]:
                        stats["angle_by_period"][period_name] = []
                    stats["angle_by_period"][period_name].append(angle)

                    # ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡çµ±è¨ˆ
                    buffer_efficiency = result["analysis"]["detection_quality"][
                        "detection_metrics"
                    ]["buffer_efficiency"]
                    if period_name not in stats["buffer_efficiency_by_period"]:
                        stats["buffer_efficiency_by_period"][period_name] = []
                    stats["buffer_efficiency_by_period"][period_name].append(
                        buffer_efficiency
                    )

                    # ç·ãƒãƒƒãƒ•ã‚¡ãƒã‚¤ãƒ³ãƒˆ
                    total_buffer = result["analysis"]["detection_quality"][
                        "buffer_effectiveness"
                    ]["total_buffer_points"]
                    stats["total_buffer_points"] += total_buffer
                else:
                    stats["period_detections"][period_name] = False

            # æ¤œå‡ºç‡è¨ˆç®—
            stats["detection_rate"] = stats["detection_count"] / stats["total_periods"]

            # æœŸé–“åˆ¥å¹³å‡å€¤è¨ˆç®—
            for period_name in stats["confidence_by_period"]:
                stats["confidence_by_period"][period_name] = sum(
                    stats["confidence_by_period"][period_name]
                ) / len(stats["confidence_by_period"][period_name])

            for period_name in stats["slope_by_period"]:
                stats["slope_by_period"][period_name] = sum(
                    stats["slope_by_period"][period_name]
                ) / len(stats["slope_by_period"][period_name])

            for period_name in stats["angle_by_period"]:
                stats["angle_by_period"][period_name] = sum(
                    stats["angle_by_period"][period_name]
                ) / len(stats["angle_by_period"][period_name])

            for period_name in stats["buffer_efficiency_by_period"]:
                stats["buffer_efficiency_by_period"][period_name] = sum(
                    stats["buffer_efficiency_by_period"][period_name]
                ) / len(stats["buffer_efficiency_by_period"][period_name])

            return stats

        except Exception as e:
            logger.error(f"æ™‚é–“è¶³åˆ¥çµ±è¨ˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _analyze_pattern_statistics(self, pattern_results: Dict) -> Dict:
        """æ¥µé™ç·©å’Œãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµ±è¨ˆåˆ†æ"""
        try:
            stats = {
                "total_timeframes": len(pattern_results),
                "total_detections": 0,
                "overall_detection_rate": 0.0,
                "timeframe_detection_summary": {},
                "best_performing_timeframe": None,
                "highest_confidence": 0.0,
                "average_slope": 0.0,
                "average_buffer_efficiency": 0.0,
                "total_buffer_points": 0,
                "monthly_estimate": 0.0,
            }

            total_periods = 0
            timeframe_performance = {}
            all_slopes = []
            all_buffer_efficiencies = []

            for timeframe, timeframe_results in pattern_results.items():
                if timeframe == "statistics":
                    continue

                timeframe_stats = timeframe_results.get("statistics", {})
                detection_count = timeframe_stats.get("detection_count", 0)
                total_periods_tf = timeframe_stats.get("total_periods", 0)

                stats["total_detections"] += detection_count
                total_periods += total_periods_tf

                detection_rate = timeframe_stats.get("detection_rate", 0.0)
                timeframe_performance[timeframe] = detection_rate

                stats["timeframe_detection_summary"][timeframe] = {
                    "detection_count": detection_count,
                    "total_periods": total_periods_tf,
                    "detection_rate": detection_rate,
                }

                # å‚¾ãã¨ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡ã®åé›†
                slope_by_period = timeframe_stats.get("slope_by_period", {})
                buffer_efficiency_by_period = timeframe_stats.get("buffer_efficiency_by_period", {})

                for period_name, slopes in slope_by_period.items():
                    all_slopes.extend(slopes)

                for period_name, efficiencies in buffer_efficiency_by_period.items():
                    all_buffer_efficiencies.extend(efficiencies)

                # ç·ãƒãƒƒãƒ•ã‚¡ãƒã‚¤ãƒ³ãƒˆ
                total_buffer = timeframe_stats.get("total_buffer_points", 0)
                stats["total_buffer_points"] += total_buffer

                # æœ€é«˜ä¿¡é ¼åº¦ã®è¿½è·¡
                confidence_by_period = timeframe_stats.get("confidence_by_period", {})
                for period_name, confidences in confidence_by_period.items():
                    if confidences and max(confidences) > stats["highest_confidence"]:
                        stats["highest_confidence"] = max(confidences)

            # å…¨ä½“æ¤œå‡ºç‡
            if total_periods > 0:
                stats["overall_detection_rate"] = stats["total_detections"] / total_periods

            # æœ€é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ™‚é–“è¶³
            if timeframe_performance:
                stats["best_performing_timeframe"] = max(
                    timeframe_performance, key=timeframe_performance.get
                )

            # å¹³å‡å‚¾ãã¨ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡
            if all_slopes:
                stats["average_slope"] = sum(all_slopes) / len(all_slopes)
            if all_buffer_efficiencies:
                stats["average_buffer_efficiency"] = sum(all_buffer_efficiencies) / len(
                    all_buffer_efficiencies
                )

            # æœˆé–“æ¨å®š
            stats["monthly_estimate"] = stats["total_detections"] / 12

            return stats

        except Exception as e:
            logger.error(f"æ¥µé™ç·©å’Œãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _get_slope_description(self, slope: float) -> str:
        """å‚¾ãã®èª¬æ˜ã‚’å–å¾—"""
        abs_slope = abs(slope)
        if abs_slope < 0.0001:
            return "ã»ã¼æ°´å¹³"
        elif abs_slope < 0.001:
            return "ç·©ã‚„ã‹ãªä¸Šæ˜‡" if slope > 0 else "ç·©ã‚„ã‹ãªä¸‹é™"
        elif abs_slope < 0.01:
            return "ä¸­ç¨‹åº¦ã®ä¸Šæ˜‡" if slope > 0 else "ä¸­ç¨‹åº¦ã®ä¸‹é™"
        elif abs_slope < 0.1:
            return "æ€¥ãªä¸Šæ˜‡" if slope > 0 else "æ€¥ãªä¸‹é™"
        else:
            return "éå¸¸ã«æ€¥ãªä¸Šæ˜‡" if slope > 0 else "éå¸¸ã«æ€¥ãªä¸‹é™"

    def _get_angle_description(self, angle: float) -> str:
        """è§’åº¦ã®èª¬æ˜ã‚’å–å¾—"""
        abs_angle = abs(angle)
        if abs_angle < 5:
            return "ã»ã¼æ°´å¹³"
        elif abs_angle < 15:
            return "ç·©ã‚„ã‹ãªä¸Šæ˜‡" if angle > 0 else "ç·©ã‚„ã‹ãªä¸‹é™"
        elif abs_angle < 30:
            return "ä¸­ç¨‹åº¦ã®ä¸Šæ˜‡" if angle > 0 else "ä¸­ç¨‹åº¦ã®ä¸‹é™"
        elif abs_angle < 45:
            return "æ€¥ãªä¸Šæ˜‡" if angle > 0 else "æ€¥ãªä¸‹é™"
        else:
            return "éå¸¸ã«æ€¥ãªä¸Šæ˜‡" if angle > 0 else "éå¸¸ã«æ€¥ãªä¸‹é™"

    async def _fetch_market_data(self, days: int) -> pd.DataFrame:
        """å¸‚å ´ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        try:
            async with db_manager.get_session() as session:
                query = text(
                    """
                    SELECT 
                        timestamp as Date,
                        open_price as Open,
                        high_price as High,
                        low_price as Low,
                        close_price as Close,
                        volume as Volume
                    FROM price_data 
                    WHERE currency_pair = 'USD/JPY'
                    ORDER BY timestamp DESC
                    LIMIT :days
                    """
                )

                result = await session.execute(query, {"days": days})
                rows = result.fetchall()

                if not rows:
                    return pd.DataFrame()

                data = pd.DataFrame(
                    rows, columns=["Date", "Open", "High", "Low", "Close", "Volume"]
                )

                data = data.sort_values("Date").reset_index(drop=True)
                return data

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    tester = Pattern15V3ExtremeRelaxationTester()
    results = await tester.test_extreme_relaxation()
    
    if "error" in results:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {results['error']}")
        return
    
    print("\n=== ãƒ‘ã‚¿ãƒ¼ãƒ³15 V3 æ¥µé™ç·©å’Œãƒ†ã‚¹ãƒˆçµæœ ===")
    
    # æ¥µé™ç·©å’Œçµæœ
    relaxation_results = results.get("relaxation_results", {})
    print(f"\nğŸ”§ æ¥µé™ç·©å’Œè¨­å®šæ¯”è¼ƒçµæœ:")
    
    for pattern_name, pattern_results in relaxation_results.items():
        print(f"\n  {pattern_name}:")
        
        pattern_stats = pattern_results.get("statistics", {})
        print(f"    ç·æ¤œå‡ºä»¶æ•°: {pattern_stats.get('total_detections', 0)}")
        print(f"    å…¨ä½“æ¤œå‡ºç‡: {pattern_stats.get('overall_detection_rate', 0):.1%}")
        print(f"    æœˆé–“æ¨å®š: {pattern_stats.get('monthly_estimate', 0):.1f}ä»¶/æœˆ")
        print(f"    æœ€é«˜ä¿¡é ¼åº¦: {pattern_stats.get('highest_confidence', 0):.3f}")
        print(f"    å¹³å‡å‚¾ã: {pattern_stats.get('average_slope', 0):.6f}")
        print(f"    å¹³å‡ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡: {pattern_stats.get('average_buffer_efficiency', 0):.3f}")
        print(f"    ç·ãƒãƒƒãƒ•ã‚¡ãƒã‚¤ãƒ³ãƒˆ: {pattern_stats.get('total_buffer_points', 0)}")
        
        # æ™‚é–“è¶³åˆ¥çµæœ
        for timeframe, timeframe_data in pattern_results.items():
            if timeframe == "statistics":
                continue
                
            tf_stats = timeframe_data.get("statistics", {})
            print(f"      {timeframe}: {tf_stats.get('detection_count', 0)}ä»¶ ({tf_stats.get('detection_rate', 0):.1%})")
            
            # è©³ç´°çµæœ
            for period_name, result in timeframe_data.items():
                if period_name == "statistics":
                    continue
                    
                if result.get("detected", False):
                    analysis = result.get("analysis", {})
                    detection_quality = analysis.get("detection_quality", {})
                    
                    print(f"        {period_name}:")
                    print(f"          å‚¾ã: {analysis['mathematical']['slope']:.6f} ({analysis['mathematical']['slope_description']})")
                    print(f"          è§’åº¦: {analysis['mathematical']['angle']:.2f}åº¦")
                    print(f"          ãƒãƒƒãƒ•ã‚¡åŠ¹ç‡: {detection_quality.get('detection_metrics', {}).get('buffer_efficiency', 0):.3f}")
                    print(f"          ãƒãƒƒãƒ•ã‚¡ã‚«ãƒãƒ¬ãƒƒã‚¸: {detection_quality.get('buffer_effectiveness', {}).get('buffer_coverage', 0):.1%}")


if __name__ == "__main__":
    asyncio.run(main())
